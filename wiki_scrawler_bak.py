#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ‰¹æ¬¡æŠ“å–ä¸­æ–‡ç¶­åŸºç™¾ç§‘æ¢ç›®ï¼ˆä»¥ã€Œè‡ºç£æ­£é«” zh-TWã€è®Šé«”è¼¸å‡ºï¼‰
- ç›®æ¨™æ¸…å–®å¯ç‚º .txtï¼ˆæ¯è¡Œä¸€å€‹ URL æˆ–æ¨™é¡Œï¼‰æˆ– .jsonlï¼ˆå« {title|url}ï¼‰
- ä¸ä½¿ç”¨ OpenCCï¼›å®Œå…¨ä¾ MediaWiki èªè¨€è®Šé«”ï¼ˆAccept-Language / variantï¼‰å–å¾—å…§å®¹
- å·²å­˜åœ¨è¼¸å‡ºè‡ªå‹•ç•¥éï¼Œå¯æ–·é»çºŒæŠ“
- æ”¯æ´æ’é™¤æŒ‡å®šç« ç¯€ï¼ˆé è¨­ï¼šåƒè€ƒè³‡æ–™,å¤–éƒ¨é€£çµ,ç›¸é—œæ¢ç›®,æ“´å±•é–±è®€,å»¶ä¼¸é–±è®€,åƒè¦‹ï¼‰
- æ®µè½åŒ–è¼¸å‡ºï¼šH2 ä¸Šä¸‹å„ç•™ 1 ç©ºè¡Œï¼›å…¶é¤˜å–®è¡Œç›¸æ¥ï¼›æ¸…å–®é …ç›®ç”¨ã€Œâ€¢ ã€
- è‡ªå‹•ä¿®è£œã€Œ<æ¨™ç±¤>ï¼š<å€¼>ã€ç¼ºå€¼ï¼ˆè‹±èª/å­¸å/è—å/æœ¬å/åŸå/åˆ¥åâ€¦ï¼‰ï¼Œæ®µå…§ DOM æ“·å– + langlinks(en/ja/.../la) å…œåº•
- è‡ªå‹•ç§»é™¤ CJK èˆ‡æ¨™é»å‘¨é‚Šå¤šé¤˜ç©ºç™½ï¼Œä¿ç•™è‹±æ–‡å–®å­—å…§ç©ºç™½
"""

import argparse
import json
import re
import time
from pathlib import Path
from typing import Iterable, Tuple, Optional
from urllib.parse import urlparse, unquote, quote

import requests
from bs4 import BeautifulSoup, Tag, NavigableString

API = "https://zh.wikipedia.org/w/api.php"
REST_HTML = "https://zh.wikipedia.org/api/rest_v1/page/html/{title}"

DEFAULT_UA = "YourBotName/1.0 (contact@example.com)"  # å»ºè­°æ›æˆä½ çš„è³‡è¨Š


# -------------------------------
# è®€å–ç›®æ¨™æ¸…å–®
# -------------------------------
def iter_targets(path: Path) -> Iterable[Tuple[str, str]]:
    """
    è®€å– .txt æˆ– .jsonl ç›®æ¨™æ¸…å–®ã€‚
    ç”¢å‡º (raw, kind)ï¼škind = "url" æˆ– "title"
    """
    if path.suffix.lower() == ".jsonl":
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                obj = json.loads(line)
                if "url" in obj and obj["url"]:
                    yield obj["url"], "url"
                elif "title" in obj and obj["title"]:
                    yield obj["title"], "title"
    else:
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                s = line.strip()
                if not s or s.startswith("#"):
                    continue
                if s.startswith("http://") or s.startswith("https://"):
                    yield s, "url"
                else:
                    yield s, "title"


# -------------------------------
# å·¥å…·å‡½æ•¸
# -------------------------------
def url_to_title(url: str) -> str:
    path = urlparse(url).path
    title = unquote(path.rsplit("/", 1)[-1])
    return title

def safe_filename(title: str) -> str:
    name = re.sub(r'[\\/*?:"<>|]', "_", title)
    return name[:200]

def http_get_with_backoff(session: requests.Session, url: str, *, params=None,
                          timeout: int = 30, retries: int = 3, backoff_base: float = 2.0):
    last_exc: Optional[Exception] = None
    for i in range(retries + 1):
        try:
            r = session.get(url, params=params, timeout=timeout)
            if r.status_code in (429, 503):
                raise RuntimeError(f"HTTP {r.status_code}")
            # Parsoid / API ä¹Ÿå¯èƒ½ä»¥å­—ä¸²æç¤º maxlagï¼Œä¿å®ˆè™•ç†
            if r.status_code == 200 and "maxlag" in r.text.lower() and "error" in r.text.lower():
                raise RuntimeError("Server under high replication lag (maxlag).")
            r.raise_for_status()
            return r
        except Exception as e:
            last_exc = e
            if i == retries:
                break
            time.sleep(backoff_base ** i)  # 2,4,8...
    raise RuntimeError(f"GET failed for {url}: {last_exc}") from last_exc


# -------------------------------
# æŠ“å–ï¼ˆREST å„ªå…ˆï¼ŒAction API å¾Œå‚™ï¼‰
# -------------------------------
def fetch_html_rest(title: str, session: requests.Session, timeout=30) -> str:
    url = REST_HTML.format(title=quote(title, safe=""))
    r = http_get_with_backoff(session, url, timeout=timeout)
    return r.text

def detect_redirect(text: str) -> Optional[str]:
    """
    æª¢æ¸¬é‡å®šå‘é é¢ï¼Œè¿”å›é‡å®šå‘çš„ç›®æ¨™æ¨™é¡Œ
    """
    # æª¢æ¸¬é‡å®šå‘æ¨™è¨˜
    redirect_patterns = [
        r"é‡å®šå‘åˆ°ï¼š\s*â€¢\s*([^\nâ€¢]+)",
        r"é‡æ–°å°å‘è‡³ï¼š\s*â€¢\s*([^\nâ€¢]+)", 
        r"#REDIRECT\s*\[\[([^\]]+)\]\]",
        r"#é‡å®šå‘\s*\[\[([^\]]+)\]\]"
    ]
    
    for pattern in redirect_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            target = match.group(1).strip()
            # æ¸…ç†å¯èƒ½çš„é¡å¤–æ¨™è¨˜
            target = re.sub(r'\|.*$', '', target)  # ç§»é™¤ç®¡é“é€£çµå¾Œçš„éƒ¨åˆ†
            return target
    
    return None


def fetch_html_action(title: str, session: requests.Session, timeout=30) -> Tuple[str, str]:
    """
    æŠ“å–é é¢å…§å®¹ï¼ŒåŒæ™‚è¿”å›å¯¦éš›çš„æ¨™é¡Œï¼ˆè™•ç†é‡å®šå‘ï¼‰
    è¿”å›: (html_content, actual_title)
    """
    params = {
        "action": "parse",
        "page": title,
        "prop": "text",
        "format": "json",
        "variant": "zh-tw",  # æŒ‡å®šè‡ºç£æ­£é«”è®Šé«”
        "maxlag": 5,
    }
    r = http_get_with_backoff(session, API, params=params, timeout=timeout)
    data = r.json()
    if "parse" not in data or "text" not in data["parse"]:
        raise RuntimeError(f"Action parse missing content for: {title}")
    
    # ç²å–å¯¦éš›çš„é é¢æ¨™é¡Œï¼ˆè™•ç†é‡å®šå‘ï¼‰
    actual_title = data["parse"].get("title", title)
    html_content = data["parse"]["text"]["*"]
    
    return html_content, actual_title


# -------------------------------
# æ’ç‰ˆæ•´ç†ï¼ˆTidyï¼‰
# -------------------------------
def zh_tidy(text: str) -> str:
    """
    ä¸­è‹±æ··æ’ç©ºç™½èˆ‡æ¨™é»æ•´ç†ï¼ˆä¿ç•™è‹±æ–‡å–®è©å…§ç©ºç™½ï¼›åªä¿®å‰ª CJK èˆ‡æ¨™é»é™„è¿‘ï¼‰ï¼š
    - æ›¸åè™Ÿ/å…§æ›¸åè™Ÿ/å¼•è™Ÿ/æ‹¬è™Ÿã€Œå…§å´ã€å»ç©ºç™½ï¼šã€Š ä½œå“ ã€‹â†’ã€Šä½œå“ã€‹ã€ã€ˆ æ—¥ä¸è½ ã€‰â†’ã€ˆæ—¥ä¸è½ã€‰ã€ï¼ˆ è‹±èªï¼šJolin Tsai ï¼‰â†’ï¼ˆè‹±èªï¼šJolin Tsaiï¼‰
    - æ¨™é»ã€Œå‰ã€å»ç©ºç™½ï¼šâ€¦â€¦ Jolin Tsai ï¼Œâ†’ â€¦â€¦ Jolin Tsaiï¼Œ
    - æ—¥æœŸã€Œå¹´/æœˆ/æ—¥ã€å»ç©ºç™½ï¼š1980å¹´ 9æœˆ 15æ—¥ â†’ 1980å¹´9æœˆ15æ—¥
    - ç ´æŠ˜è™Ÿ/å…©å­—ç ´æŠ˜è™Ÿå‘¨é‚Šå»ç©ºç™½ï¼š â€” â†’ â€”ã€ â€”â€” â†’ â€”â€”
    - CJK èˆ‡ CJK ä¹‹é–“çš„å¤šé¤˜ç©ºç™½å»é™¤ï¼›ã€Œã€ã€å‘¨é‚Šå»ç©ºç™½
    - å£“æ‰ 3 é€£ä»¥ä¸Šç©ºç™½è¡Œ
    """
    # 0) æ¨™æº–åŒ–ä¸å¯è¦‹ç©ºç™½
    text = re.sub(r"[ \t\u00A0]+", " ", text)

    # 1) æ‹¬è™Ÿ/æ›¸åè™Ÿ/å¼•è™Ÿ å…§å´ç©ºç™½
    pairs = [
        ("ã€Š", "ã€‹"), ("ã€ˆ", "ã€‰"),
        ("ã€Œ", "ã€"), ("ã€", "ã€"),
        ("ï¼ˆ", "ï¼‰")
    ]
    for l, r in pairs:
        text = re.sub(fr"{re.escape(l)}\s*(.*?)\s*{re.escape(r)}", fr"{l}\1{r}", text, flags=re.S)

    # 2) æ¨™é»ã€Œå‰ã€ä¸ç•™ç©ºç™½ï¼ˆä¸­æ–‡æ¨™é»ï¼‰
    text = re.sub(r"\s+([ï¼Œã€‚ã€ï¼›ï¼šï¼ï¼Ÿã€‹ï¼‰ã€ã€])", r"\1", text)

    # 3) æ—¥æœŸã€Œå¹´/æœˆ/æ—¥ã€ä¹‹é–“ä¸ç•™ç©ºç™½
    text = re.sub(r"(\d{1,4})\s*å¹´\s*(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥", r"\1å¹´\2æœˆ\3æ—¥", text)
    text = re.sub(r"(\d{1,4})\s*å¹´\s*(\d{1,2})\s*æœˆ", r"\1å¹´\2æœˆ", text)

    # 4) ç ´æŠ˜è™Ÿå‘¨é‚Šä¸ç•™ç©ºç™½ï¼ˆâ€” èˆ‡ â€”â€”ï¼‰
    text = re.sub(r"\s*â€”â€”\s*", "â€”â€”", text)
    text = re.sub(r"\s*â€”\s*", "â€”", text)

    # 5) CJK èˆ‡ CJK ä¹‹é–“å¤šé¤˜ç©ºç™½å»é™¤ï¼›ã€Œã€ã€å‘¨é‚Šå»ç©ºç™½
    CJK = r"\u4E00-\u9FFF\u3400-\u4DBF"
    # æ™ºèƒ½è™•ç†ï¼šä¿ç•™æ¨™é¡Œé–“çš„æ›è¡Œï¼Œä½†å»é™¤æ®µè½å…§çš„å¤šé¤˜ç©ºç™½
    # å…ˆè™•ç†æ¨™é¡Œè¡Œï¼ˆçŸ­è¡Œä¸”ç¨ç«‹ï¼‰
    lines = text.split('\n')
    for i, line in enumerate(lines):
        stripped = line.strip()
        # å¦‚æœæ˜¯çŸ­è¡Œï¼ˆå¯èƒ½æ˜¯æ¨™é¡Œï¼‰ï¼Œä¿æŒå–®ç¨æˆè¡Œ
        if stripped and len(stripped) < 30 and not any(char in stripped for char in 'ã€‚ï¼Œï¼ï¼Ÿ'):
            continue
        # å°æ–¼é•·æ®µè½ï¼Œå»é™¤ä¸­æ–‡å­—ç¬¦é–“çš„ç©ºæ ¼
        if stripped:
            lines[i] = re.sub(fr"(?<=[{CJK}])[ \t\u00A0]+(?=[{CJK}])", "", stripped)
    
    text = '\n'.join(lines)
    text = re.sub(r"\s*ã€\s*", "ã€", text)

    # 6) é€£çºŒ 3 è¡Œä»¥ä¸Šç©ºç™½ â†’ 2 è¡Œï¼ˆæœ€å¤šåªå…è¨±å…©å€‹æ›è¡Œï¼Œç”¨æ–¼å¤§æ¨™åˆ†éš”ï¼‰
    text = re.sub(r"\n{3,}", "\n\n", text)

    return text


# -------------------------------
# æ–‡å­—æŠ½å–ï¼ˆREST èˆ‡ Action çš†æ”¯æ´ï¼‰
# -------------------------------
def smart_text(node: Tag) -> str:
    """
    é€æ–‡å­—ç¯€é»ä¸²æ¥ï¼š
    - åªæœ‰ã€Œä¸Šä¸€å€‹å­—å…ƒã€èˆ‡ã€Œç•¶å‰ç‰‡æ®µç¬¬ä¸€å€‹å­—å…ƒã€éƒ½ç‚º ASCII å­—æ¯/æ•¸å­—æ™‚ï¼Œæ‰è£œ 1 å€‹ç©ºç™½
    - å…¶ä»–æƒ…æ³ç›´æ¥ç›¸é€£ï¼Œé¿å…åœ¨ CJK é‚Šç•Œè£½é€ å¤šé¤˜ç©ºç™½
    """
    parts = []
    last_ascii = False
    for d in node.descendants:
        if isinstance(d, NavigableString):
            s = str(d)
            if not s or not s.strip():
                continue
            s = s.strip()
            cur_ascii = bool(s and s[0].isascii())
            if parts and last_ascii and cur_ascii:
                parts.append(" ")
            parts.append(s)
            last_ascii = bool(s and s[-1].isascii())
        elif isinstance(d, Tag) and d.name == "br":
            parts.append("\n")
            last_ascii = False
    return "".join(parts)


# ===== é€šç”¨æ¨™ç±¤è™•ç†ï¼ˆå­¸å / è—å / æœ¬å / åŸå / åˆ¥å / å¤šèªè¨€ï¼‰ =====

# å¸¸è¦‹æ¨™ç±¤ â†’ BCP-47 èªè¨€ç¢¼ï¼ˆå¯è‡ªè¡Œæ“´å……ï¼‰
LABEL_LANG_MAP = {
    # èªè¨€åç¨±ï¼ˆå«å¸¸è¦‹åˆ¥ç¨±ï¼‰
    "è‹±èª": "en", "è‹±æ–‡": "en", "English": "en",
    "æ—¥èª": "ja", "æ—¥æ–‡": "ja", "Japanese": "ja",
    "éŸ“èª": "ko", "éŸ“æ–‡": "ko",
    "æ³•èª": "fr", "æ³•æ–‡": "fr",
    "å¾·èª": "de", "å¾·æ–‡": "de",
    "è¥¿ç­ç‰™èª": "es", "è¥¿æ–‡": "es", "è¥¿èª": "es",
    "ä¿„èª": "ru", "ä¿„æ–‡": "ru",
    "ç¾©å¤§åˆ©èª": "it", "ç¾©æ–‡": "it", "æ„å¤§åˆ©èª": "it", "æ„æ–‡": "it",
    "è‘¡è„ç‰™èª": "pt", "è‘¡æ–‡": "pt",
    "æ‹‰ä¸èª": "la", "æ‹‰ä¸æ–‡": "la",
    "è¶Šå—èª": "vi", "è¶Šæ–‡": "vi",
    "æ³°èª": "th",
    "é¦¬ä¾†èª": "ms", "é¦¬ä¾†æ–‡": "ms",
    "å°å°¼èª": "id", "å°å°¼æ–‡": "id",
    "ç²µèª": "yue", "å»£æ±è©±": "yue",
    "é–©å—èª": "nan", "è‡ºèª": "nan", "å°èª": "nan", "é–©å—è©±": "nan",
    "å®¢èª": "hak",
    # éèªè¨€ä½†å¸¸è¦‹ï¼šå­¸åï¼ˆå¤šåŠæ‹‰ä¸ï¼‰
    "å­¸å": "la",
}

# æœƒè¢«ç•¶æˆã€Œå€¼ç¼ºå¤±ã€çš„åˆ†éš”ç¬¦ï¼ˆé‡åˆ°å®ƒå€‘å°±åœæ­¢è’é›†ï¼‰
_STOP_CHARS = "ï¼Œ,ã€ï¼›;,)ï¼‰ã€ã€ã€‘ã€‰ã€‚."

def _fetch_langlink_title(title: str, session: requests.Session, lang_code: str) -> Optional[str]:
    """
    ç”¨ MediaWiki Action API è®€å–äº’èªé€£çµæ¨™é¡Œï¼ˆlllang=<code>ï¼‰
    """
    params = {
        "action": "query",
        "prop": "langlinks",
        "titles": title,
        "lllang": lang_code,
        "format": "json",
    }
    try:
        r = http_get_with_backoff(session, API, params=params, timeout=20)
        data = r.json()
        pages = data.get("query", {}).get("pages", {})
        for _, page in pages.items():
            for ll in page.get("langlinks", []) or []:
                if ll.get("lang") == lang_code:
                    return ll.get("*") or ll.get("title")
    except Exception:
        pass
    return None


def _collect_value_after_label(el: Tag, label: str, maxlen: int = 120) -> Optional[str]:
    """
    åœ¨æ®µè½ el è£¡æ‰¾åˆ°ã€Œ<label>ï¼šã€é€™å€‹é‚è¼¯åºåˆ—ï¼ˆå…è¨± label èˆ‡å†’è™Ÿè·¨ç¯€é»ï¼‰ï¼Œ
    ä¹‹å¾ŒæŠŠå€¼ä¸€è·¯æ”¶é›†åˆ°é‡åˆ°åœæ­¢å­—å…ƒç‚ºæ­¢ã€‚
    ä¾‹ï¼š<a>å­¸å</a>ï¼š<i><span lang="la">Oncorhynchus masou formosanus</span></i>
    """
    # 1) æ‰¾åˆ°æ–‡å­—å‰›å¥½ç­‰æ–¼ label çš„ç¯€é»ï¼ˆå¯èƒ½æ˜¯ NavigableString æˆ– <a>/<b>/<span> ç­‰ Tagï¼‰
    anchor: Optional[object] = None
    for d in el.descendants:
        if isinstance(d, NavigableString):
            if d.strip() == label:
                anchor = d
                break
        elif isinstance(d, Tag):
            # åƒ…å–ç´”æ–‡å­—å‰›å¥½ç­‰æ–¼ label çš„ç¯€é»ï¼Œé¿å…æŠ“åˆ°é•·å¥å­ä¸­çš„ç‰‡æ®µ
            if d.get_text("", strip=True) == label:
                anchor = d
                break
    if not anchor:
        return None

    # 2) å¾ anchor å¾€å¾Œæ‰¾ã€Œç¬¬ä¸€å€‹å†’è™Ÿã€ï¼ˆå…è¨±å…¨/åŠå½¢ï¼‰ï¼Œåƒæ‰å®ƒä¹‹å¾Œé–‹å§‹æ”¶é›†å€¼
    it = anchor.next_elements
    saw_colon = False
    parts: list[str] = []

    def cut_at_stop(s: str) -> tuple[str, bool]:
        """æŠŠ s ç åˆ°ç¬¬ä¸€å€‹åœæ­¢å­—å…ƒï¼Œå›å‚³ (æœ‰æ•ˆå…§å®¹, æ˜¯å¦é‡åˆ°åœæ­¢)"""
        s = s.strip()
        if not s:
            return "", False
        for idx, ch in enumerate(s):
            if ch in _STOP_CHARS:
                return s[:idx], True
        return s, False

    # 3) å¯èƒ½å­˜åœ¨é€™ç¨®æƒ…æ³ï¼šlabel çš„**ä¸‹ä¸€å€‹**æ–‡å­—ç¯€é»ä¸€é–‹å§‹å°±æ˜¯ã€Œï¼šã€æˆ–å«ã€Œï¼š...ã€
    for node in it:
        if isinstance(node, NavigableString):
            s = str(node)
            if not saw_colon:
                pos = s.find("ï¼š")
                if pos < 0:
                    pos = s.find(":")  # ä¹Ÿå®¹è¨±åŠå½¢å†’è™Ÿ
                if pos < 0:
                    # é‚„æ²’çœ‹åˆ°å†’è™Ÿï¼Œç•¥é
                    continue
                # åƒæ‰å†’è™Ÿï¼Œå†’è™Ÿå¾Œå¯èƒ½ç•¶å ´å°±æœ‰å€¼
                s = s[pos + 1:]
                saw_colon = True

            # å·²çœ‹åˆ°å†’è™Ÿï¼Œé–‹å§‹æ”¶é›†å€¼ï¼Œç›´åˆ°é‡åˆ°åœæ­¢å­—å…ƒ
            chunk, stop = cut_at_stop(s)
            if chunk:
                parts.append(chunk)
            if stop:
                break

        elif isinstance(node, Tag):
            # Tag å…§çš„æ–‡å­—æœƒåœ¨å…¶ NavigableString å­å­«ç¯€é»è¢«è™•ç†ï¼›
            # ä½†è‹¥æ˜¯ <br>ï¼Œè¦–ç‚ºç¡¬æ›è¡Œä¸æ”¶é›†
            if node.name == "br":
                break

        # å®‰å…¨ä¸Šé™ï¼Œé¿å…ç•°å¸¸é•·åº¦
        if sum(len(p) for p in parts) > maxlen * 2:
            break

    val = " ".join(p for p in parts if p).strip()
    return val[:maxlen] if val else None

def _collect_value_after_marker(el: Tag, marker_text: str, maxlen: int = 120) -> Optional[str]:
    """
    å¾å«æœ‰ marker_textï¼ˆå¦‚ã€Œå­¸åï¼šã€ã€Œè‹±èªï¼šã€ï¼‰çš„æ®µè½ el å…§ï¼Œ
    æ²¿ DOM next_elements é€£çºŒè’é›†æ–‡å­—ï¼Œç›´åˆ°é‡åˆ° _STOP_CHARSã€‚
    """
    needle = None
    for d in el.descendants:
        if isinstance(d, NavigableString):
            s = str(d)
            if marker_text in s:
                needle = d
                break
    if not needle:
        return None

    parts: list[str] = []
    tail = str(needle).split(marker_text, 1)[1]

    def consume(s: str) -> tuple[str, bool]:
        s = s.strip()
        if not s:
            return "", False
        indices = [i for i, ch in enumerate(s) if ch in _STOP_CHARS]
        if indices:
            i = min(indices)
            return s[:i], True
        return s, False

    if tail:
        chunk, stop = consume(tail)
        if chunk:
            parts.append(chunk)
        if stop:
            out = " ".join(p for p in parts if p).strip()
            return out[:maxlen] if out else None

    for node in needle.next_elements:
        if isinstance(node, NavigableString):
            chunk, stop = consume(str(node))
            if chunk:
                parts.append(chunk)
            if stop:
                break
        elif isinstance(node, Tag):
            classes = " ".join(node.get("class", [])).lower()
            if "reference" in classes or "mw-editsection" in classes:
                continue
        if sum(len(p) for p in parts) > maxlen * 1.5:
            break

    out = " ".join(p for p in parts if p).strip(" ")
    return out[:maxlen] if out else None


# æ¨™ç±¤æ ¸å¿ƒé›†åˆï¼ˆèªè¨€ + å¸¸è¦‹åˆ¥åé¡æ¨™ç±¤ï¼‰
_LABEL_CORE_RE = r"(è‹±èª|è‹±æ–‡|English|æ—¥èª|æ—¥æ–‡|Japanese|éŸ“èª|éŸ“æ–‡|æ³•èª|æ³•æ–‡|å¾·èª|å¾·æ–‡|è¥¿ç­ç‰™èª|è¥¿æ–‡|è¥¿èª|ä¿„èª|ä¿„æ–‡|ç¾©å¤§åˆ©èª|ç¾©æ–‡|æ„å¤§åˆ©èª|æ„æ–‡|è‘¡è„ç‰™èª|è‘¡æ–‡|æ‹‰ä¸èª|æ‹‰ä¸æ–‡|è¶Šå—èª|è¶Šæ–‡|æ³°èª|é¦¬ä¾†èª|é¦¬ä¾†æ–‡|å°å°¼èª|å°å°¼æ–‡|ç²µèª|å»£æ±è©±|é–©å—èª|è‡ºèª|å°èª|é–©å—è©±|å®¢èª|å­¸å|è—å|æœ¬å|åŸå|èˆŠç¨±|åˆå|åˆ¥å|åˆ¥ç¨±|å¤–æ–‡)"

# åµæ¸¬ã€Œæ¨™ç±¤ï¼šã€ä¹‹å¾Œæ¥åˆ†éš”ç¬¦ï¼ˆä»£è¡¨ç¼ºå€¼ï¼‰
_MISSING_AFTER_LABEL = re.compile(_LABEL_CORE_RE + r"ï¼š(?=\s*[ï¼Œã€ï¼›)ï¼‰])")

def fill_labels_if_missing(el: Tag, txt: str) -> str:
    """
    è‹¥åµæ¸¬åˆ°ã€Œ<æ¨™ç±¤>ï¼šã€å¾Œç·Šè·Ÿåˆ†éš”ç¬¦ï¼ˆå€¼ç¼ºå¤±ï¼‰ï¼Œå…ˆç”¨ã€Œæ¨™ç±¤åˆ†ç¯€é»ã€æ³•è£œï¼Œ
    è‹¥å¤±æ•—å†ç”¨ã€Œæ•´æ®µ markerã€æ³•è£œã€‚
    """
    def _repl(m: re.Match) -> str:
        label = m.group(1)
        # å…ˆç”¨åŸæ–¹æ³•æ‰¾
        val = _collect_value_after_label(el, label)
        if not val:
            # åŒä¸€ text node çš„æ¨™ç±¤ï¼šå†’è™Ÿæƒ…æ³
            val = _collect_value_after_marker(el, f"{label}ï¼š")
        return f"{label}ï¼š{val}" if val else m.group(0)

    return _MISSING_AFTER_LABEL.sub(_repl, txt)

def ensure_labels_after_marker(text: str, *, title: str, session: requests.Session) -> str:
    """
    å…¨æ–‡å±¤ç´šæœ€çµ‚å…œåº•ï¼šå°ä»ã€Œ<èªè¨€æ¨™ç±¤>ï¼š<ç¼ºå€¼>ã€è€…ï¼Œå˜—è©¦ç”¨ langlinks å–å°æ‡‰èªè¨€æ¨™é¡Œè£œä¸Šã€‚
    åƒ…å° LABEL_LANG_MAP æœ‰å°æ‡‰ç¢¼çš„æ¨™ç±¤ç”Ÿæ•ˆï¼›å…¶ä»–ï¼ˆå¦‚ è—å/æœ¬åï¼‰ä¸åœ¨æ­¤è™•è£œã€‚
    """
    def _repl(m: re.Match) -> str:
        label = m.group(1)
        code = LABEL_LANG_MAP.get(label)
        if not code:
            return m.group(0)
        name = _fetch_langlink_title(title, session, code)
        return f"{label}ï¼š{name}" if name else m.group(0)

    return _MISSING_AFTER_LABEL.sub(_repl, text)


def html_to_text(html: str, exclude_sections: list[str] | None = None) -> str:
    """
    æ®µè½åŒ–è¼¸å‡ºï¼ˆREST / Action çš†å¯ï¼‰ï¼š
    - å™ªéŸ³å…ˆç§»é™¤
    - é€æƒ h2/h3/p/ul/olï¼›é‡åˆ°è¢«æ’é™¤ç« ç¯€çš„ h2 å¾Œç›´åˆ°ä¸‹ä¸€å€‹ h2 å…¨è·³é
    - **H2** ä¸Šä¸‹å„ç•™ 1 ç©ºç™½è¡Œï¼›å…¶é¤˜è¡Œå–®æ›è¡Œ
    - è‹¥åµæ¸¬åˆ°ã€Œ<æ¨™ç±¤>ï¼šã€å¾Œå…§å®¹ç¼ºå¤±ï¼Œå¾è©²æ®µ DOM è£¡è£œæŠ“å€¼
    - æœ€å¾Œå‘¼å« zh_tidy åšæ¨™é»èˆ‡ç©ºç™½æ”¶æ–‚
    """
    soup = BeautifulSoup(html, "html.parser")

    # å™ªéŸ³æ¸…é™¤
    for sel in [
        "sup.reference", "span.mw-editsection", "table.infobox", "table.navbox",
        "div.reflist", "ol.references", "div.metadata",
        # æ¸…é™¤ç¶­åŸºç™¾ç§‘è­¦å‘Šæ¡†å’Œç·¨è¼¯æç¤º
        "div.ambox", "div.mbox-small", "div.messagebox", "table.ambox",
        "div.hatnote", "div.dablink", "div.rellink",
        # æ¸…é™¤ infobox ç›¸é—œ
        "table.infobox", "div.infobox", ".infobox",
        # æ¸…é™¤æ¨¡æ¿å’Œç·¨è¼¯ç›¸é—œ
        "div.navbox", "table.navbox", "div.mw-collapsible"
    ]:
        for tag in soup.select(sel):
            tag.decompose()

    root = soup.select_one("div.mw-parser-output") or soup.body or soup
    elements = root.find_all(["h2", "h3", "p", "ul", "ol", "table"], recursive=True)

    exclude = set(exclude_sections or [])
    lines: list[str] = []
    skipping = False

    def norm_title(s: str) -> str:
        return re.sub(r"\[.*?\]", "", s).strip()

    def squeeze(s: str) -> str:
        return re.sub(r"[ \t\u00A0]+", " ", s.strip())

    for el in elements:
        if el.name == "h2":
            title = norm_title(smart_text(el))
            if any(key in title for key in exclude):
                skipping = True
                continue
            skipping = False
            if lines and lines[-1] != "":
                lines.append("")          # H2 å‰ç©ºè¡Œ
            if title:
                lines.append(squeeze(title))
                lines.append("")          # H2 å¾Œç©ºè¡Œ
            continue

        if el.name == "h3":
            if skipping:
                continue
            title = norm_title(smart_text(el))
            if title:
                # ç¢ºä¿H3å‰æœ‰é©ç•¶çš„åˆ†éš”ï¼ˆå¦‚æœå‰é¢ä¸æ˜¯ç©ºè¡Œçš„è©±ï¼‰
                if lines and lines[-1] != "":
                    lines.append("")
                lines.append(squeeze(title))
            continue

        if skipping:
            continue

        if el.name == "p":
            txt = squeeze(smart_text(el))
            if txt:
                # æª¢æŸ¥æ˜¯å¦æ˜¯ç°¡çŸ­çš„æ¨™é¡Œå‹æ®µè½ï¼ˆå¦‚ã€Œå€‹äººæ¦®è­½ã€ï¼‰
                is_likely_title = (
                    len(txt) < 50 and 
                    not txt.endswith(('ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?')) and
                    not any(char in txt for char in 'ï¼Œ,ã€ï¼›;:ï¼š')
                )
                
                if is_likely_title:
                    # ç•¶ä½œæ¨™é¡Œè™•ç†
                    lines.append(txt)
                else:
                    # é€šç”¨æ¨™ç±¤è£œå€¼ï¼ˆå°±åœ° DOM æƒæï¼‰
                    txt = fill_labels_if_missing(el, txt)
                    lines.append(txt)
        elif el.name in ("ul", "ol"):
            items = []
            for li in el.find_all("li", recursive=False):
                li_txt = squeeze(smart_text(li))
                if li_txt:
                    items.append(f"â€¢ {li_txt}")
            if items:
                lines.extend(items)
        elif el.name == "table":
            # åªè™•ç†å…§å®¹è¡¨æ ¼ï¼Œè·³é infoboxã€navboxã€ambox ç­‰
            classes = " ".join(el.get("class", [])).lower()
            if any(cls in classes for cls in ["infobox", "navbox", "ambox", "mbox", "messagebox"]):
                continue
            
            # è™•ç†è¡¨æ ¼ï¼šå°‡æ¯è¡Œè½‰æ›ç‚ºã€Œâ€¢ åˆ—1 | åˆ—2 | åˆ—3ã€æ ¼å¼
            table_lines = []
            for row in el.find_all("tr"):
                cells = []
                for cell in row.find_all(["th", "td"]):
                    cell_txt = squeeze(smart_text(cell))
                    if cell_txt:
                        cells.append(cell_txt)
                if cells:
                    table_lines.append("â€¢ " + " | ".join(cells))
            if table_lines:
                lines.extend(table_lines)

    text = "\n".join(lines)
    
    # å¾Œè™•ç†ï¼šåˆ†é›¢å¯èƒ½é€£åœ¨ä¸€èµ·çš„æ¨™é¡Œ
    text = separate_concatenated_titles(text)
    text = zh_tidy(text)
    return text


def separate_concatenated_titles(text: str) -> str:
    """
    åˆ†é›¢å¯èƒ½è¢«é€£åœ¨ä¸€èµ·çš„æ¨™é¡Œï¼Œä¾‹å¦‚ã€Œå½±éŸ³ä½œå“å…¶ä»–éŸ³æ¨‚éŒ„å½±å¸¶ã€-> ã€Œå½±éŸ³ä½œå“\nå…¶ä»–éŸ³æ¨‚éŒ„å½±å¸¶ã€
    """
    # å¸¸è¦‹çš„éœ€è¦åˆ†é›¢çš„æ¨™é¡Œæ¨¡å¼
    title_pairs = [
        ("å½±éŸ³ä½œå“", "å…¶ä»–éŸ³æ¨‚éŒ„å½±å¸¶"),
        ("å€‹äººç”Ÿæ´»", "æ„Ÿæƒ…ç‹€æ³"),
        ("æ¼”è—ç¶“æ­·", "éŸ³æ¨‚ä½œå“"),
        ("ä½œå“åˆ—è¡¨", "éŸ³æ¨‚ä½œå“"),
        ("ç²çè¨˜éŒ„", "å€‹äººæ¦®è­½"),
    ]
    
    for title1, title2 in title_pairs:
        # æŸ¥æ‰¾é€£åœ¨ä¸€èµ·çš„æ¨™é¡Œ
        combined = title1 + title2
        if combined in text:
            # æ›¿æ›ç‚ºåˆ†è¡Œçš„ç‰ˆæœ¬
            text = text.replace(combined, f"{title1}\n\n{title2}")
    
    return text


# -------------------------------
# ä¸»æµç¨‹ï¼šå–®ç¯‡è™•ç†
# -------------------------------
def process_one(raw: str, kind: str, out_dir: Path, session: requests.Session,
                exclude_sections: list[str], jsonl_file) -> Tuple[str, bool, str]:
    title = url_to_title(raw) if kind == "url" else raw
    filename = safe_filename(title)
    out_txt = out_dir / f"{filename}.txt"

    if out_txt.exists():
        return title, True, "exists"

    # å…ˆ Actionï¼ˆvariant=zh-twï¼‰ï¼Œå¤±æ•—å† RESTï¼ˆå¸¶ Accept-Language: zh-twï¼‰
    html = None
    actual_title = title
    try:
        html, actual_title = fetch_html_action(title, session)
    except Exception:
        html = fetch_html_rest(title, session)

    text = html_to_text(html, exclude_sections=exclude_sections)
    
    # æª¢æ¸¬æ˜¯å¦ç‚ºé‡å®šå‘é é¢
    redirect_target = detect_redirect(text)
    if redirect_target and redirect_target != title:
        print(f"ğŸ”„ æª¢æ¸¬åˆ°é‡å®šå‘ï¼š{title} -> {redirect_target}")
        # æŠ“å–é‡å®šå‘ç›®æ¨™é é¢
        try:
            html, actual_title = fetch_html_action(redirect_target, session)
        except Exception:
            try:
                html = fetch_html_rest(redirect_target, session)
                actual_title = redirect_target
            except Exception as e:
                raise RuntimeError(f"é‡å®šå‘ç›®æ¨™é é¢æŠ“å–å¤±æ•—: {redirect_target}, éŒ¯èª¤: {e}")
        
        text = html_to_text(html, exclude_sections=exclude_sections)
        # æ›´æ–°æ–‡ä»¶åç‚ºå¯¦éš›ç›®æ¨™é é¢
        filename = safe_filename(actual_title)
        out_txt = out_dir / f"{filename}.txt"
        
        # å¦‚æœç›®æ¨™æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³é
        if out_txt.exists():
            return actual_title, True, "redirect_exists"

    # å…¨æ–‡å…œåº•ï¼šè‹¥ä»æœ‰ã€Œ<èªè¨€æ¨™ç±¤>ï¼š<ç¼ºå€¼>ã€ï¼Œç”¨ langlinks è£œ
    text = ensure_labels_after_marker(text, title=actual_title, session=session)

    out_dir.mkdir(parents=True, exist_ok=True)
    out_txt.write_text(text, encoding="utf-8")
    
    # å¯«å…¥å…¨åŸŸ jsonl æ–‡ä»¶
    rec = {
        "title": actual_title,
        "original_query": title if redirect_target else actual_title,
        "source_url": f"https://zh.wikipedia.org/wiki/{quote(actual_title, safe='')}",
        "variant": "zh-tw",
        "text_length": len(text),
        "text": text,
    }
    if redirect_target:
        rec["redirected_from"] = title
        rec["redirected_to"] = actual_title
    
    jsonl_file.write(json.dumps(rec, ensure_ascii=False) + "\n")
    jsonl_file.flush()  # ç¢ºä¿ç«‹å³å¯«å…¥
    
    status = "redirect_ok" if redirect_target else "ok"
    return actual_title, True, status


# -------------------------------
# CLI å…¥å£
# -------------------------------
def main():
    ap = argparse.ArgumentParser(description="æ‰¹æ¬¡æŠ“å–ä¸­æ–‡ç¶­åŸºï¼ˆè‡ºç£æ­£é«” zh-TWï¼‰")
    ap.add_argument("--targets", required=True, help="ç›®æ¨™æ¸…å–®è·¯å¾‘ï¼š.txt æˆ– .jsonl")
    ap.add_argument("--out-dir", default="out", help="è¼¸å‡ºè³‡æ–™å¤¾")
    ap.add_argument("--sleep", type=float, default=1.0, help="æ¯ç¯‡ä¹‹é–“çš„å»¶é²ç§’æ•¸ï¼ˆç¦®è²ŒæŠ“å–ï¼‰")
    ap.add_argument("--ua", default=DEFAULT_UA, help="è‡ªè¨‚ User-Agentï¼ˆè«‹å¡«å¯è¯çµ¡è³‡è¨Šï¼‰")
    ap.add_argument(
        "--exclude-sections",
        # zh è®Šé«”æœƒæŠŠã€Œç›¸å…³æ¡ç›®/æ‰©å±•é˜…è¯»ã€è‡ªå‹•è½‰ç‚ºã€Œç›¸é—œæ¢ç›®/æ“´å±•é–±è®€ã€
        default="åƒè€ƒæ›¸ç›®,ç›¸é—œå­¸è¡“ç ”ç©¶æ›¸ç›®,åƒè€ƒä¾†æº,åƒè€ƒè³‡æ–™,å¤–éƒ¨é€£çµ,ç›¸é—œæ¢ç›®,æ“´å±•é–±è®€,å»¶ä¼¸é–±è®€,åƒè¦‹,åƒè€ƒæ–‡ç»,è…³è¨»,è¨»é‡‹,è¨»è§£,æ³¨è§£,å‚™è¨»,é—œè¯é …ç›®,è³‡æ–™ä¾†æº,æ³¨é‡‹,è¨»è…³,æ³¨è…³,é—œé€£é …ç›®",
        help="è¦æ’é™¤çš„ç« ç¯€æ¨™é¡Œï¼ˆä»¥é€—è™Ÿåˆ†éš”ï¼‰",
    )
    args = ap.parse_args()

    targets_path = Path(args.targets)
    out_dir = Path(args.out_dir)
    exclude_sections = [s.strip() for s in args.exclude_sections.split(",") if s.strip()]

    S = requests.Session()
    S.headers.update({
        "User-Agent": args.ua,
        # è®“ REST/Parsoid ä¾è®Šé«”è¼¸å‡ºè‡ºç£æ­£é«”ï¼›èˆ‡ Action API çš„ variant ç›¸è¼”ç›¸æˆ
        "Accept-Language": "zh-tw",
    })

    ok, skip, fail = 0, 0, 0
    failures_log = out_dir / "_failures.jsonl"
    all_data_jsonl = out_dir / "all_data.jsonl"
    
    # å‰µå»ºå…¨åŸŸ jsonl æ–‡ä»¶ï¼ˆä½¿ç”¨ append æ¨¡å¼ï¼Œé¿å…æ¯æ¬¡è¦†å¯«ï¼‰
    out_dir.mkdir(parents=True, exist_ok=True)
    with all_data_jsonl.open("a", encoding="utf-8") as jsonl_file:
        for raw, kind in iter_targets(targets_path):
            try:
                title, success, msg = process_one(raw, kind, out_dir, S, exclude_sections, jsonl_file)
                if msg == "exists":
                    skip += 1
                    print(f"â­ï¸  ç•¥éï¼ˆå·²å­˜åœ¨ï¼‰ï¼š{title}")
                elif msg == "redirect_exists":
                    skip += 1
                    print(f"â­ï¸  ç•¥éï¼ˆé‡å®šå‘ç›®æ¨™å·²å­˜åœ¨ï¼‰ï¼š{title}")
                elif msg == "redirect_ok":
                    ok += 1
                    print(f"âœ… å®Œæˆï¼ˆé‡å®šå‘ï¼‰ï¼š{raw} -> {title}")
                else:
                    ok += 1
                    print(f"âœ… å®Œæˆï¼š{title}")
            except Exception as e:
                fail += 1
                print(f"âŒ  å¤±æ•—ï¼š{raw}  -> {e}")
                failures_log.parent.mkdir(parents=True, exist_ok=True)
                with failures_log.open("a", encoding="utf-8") as f:
                    f.write(json.dumps({"raw": raw, "kind": kind, "error": str(e)}, ensure_ascii=False) + "\n")
            time.sleep(args.sleep)

    print("\n=== çµ±è¨ˆ ===")
    print(f"æˆåŠŸï¼š{ok}  å·²å­˜åœ¨ï¼š{skip}  å¤±æ•—ï¼š{fail}")
    print(f"è¼¸å‡ºè³‡æ–™å¤¾ï¼š{out_dir.resolve()}")


if __name__ == "__main__":
    main()