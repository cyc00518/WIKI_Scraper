#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ‰¹æ¬¡æŠ“å–ä¸­æ–‡ç¶­åŸºç™¾ç§‘æ¢ç›®ï¼ˆä»¥ã€Œè‡ºç£æ­£é«” zh-TWã€è®Šé«”è¼¸å‡ºï¼‰
- ç›®æ¨™æ¸…å–®å¯ç‚º .txtï¼ˆæ¯è¡Œä¸€å€‹ URL æˆ–æ¨™é¡Œï¼‰æˆ– .jsonlï¼ˆå« {title|url}ï¼‰
- ä¸ä½¿ç”¨ OpenCCï¼›å®Œå…¨ä¾ MediaWiki èªè¨€è®Šé«”ï¼ˆAccept-Language / variantï¼‰å–å¾—å…§å®¹
- å·²å­˜åœ¨è¼¸å‡ºè‡ªå‹•ç•¥éï¼Œå¯æ–·é»çºŒæŠ“
- æ”¯æ´æ’é™¤æŒ‡å®šç« ç¯€ï¼ˆé è¨­ï¼šåƒè€ƒè³‡æ–™,å¤–éƒ¨é€£çµ,ç›¸é—œæ¢ç›®,æ“´å±•é–±è®€,å»¶ä¼¸é–±è®€,åƒè¦‹ï¼‰
- æ®µè½åŒ–è¼¸å‡ºï¼šH2 ä¸Šä¸‹å„ç•™ 1 ç©ºè¡Œï¼›å…¶é¤˜å–®è¡Œç›¸æ¥ï¼›æ¸…å–®é …ç›®ç”¨ã€Œâ€¢ ã€
- è‡ªå‹•ä¿®è£œã€Œ<æ¨™ç±¤>ï¼š<å€¼>ã€ç¼ºå€¼ï¼ˆè‹±èª/å­¸å/è—å/æœ¬å/åŸå/åˆ¥åâ€¦ï¼‰ï¼Œæ®µå…§ DOM æ“·å– + langlinks(en/ja/.../la) å…œåº•
- è‡ªå‹•ç§»é™¤ CJK èˆ‡æ¨™é»å‘¨é‚Šå¤šé¤˜ç©ºç™½ï¼Œä¿ç•™è‹±æ–‡å–®å­—å…§ç©ºç™½
"""

import argparse
import json
import re
import time
import hashlib
import os
from pathlib import Path
from typing import Iterable, Tuple, Optional
from urllib.parse import urlparse, unquote, quote

import requests
from bs4 import BeautifulSoup, Tag, NavigableString

API = "https://zh.wikipedia.org/w/api.php"
REST_HTML = "https://zh.wikipedia.org/api/rest_v1/page/html/{title}"

DEFAULT_UA = "YourBotName/1.0 (contact@example.com)"  # å»ºè­°æ›æˆä½ çš„è³‡è¨Š


# -------------------------------
# è®€å–ç›®æ¨™æ¸…å–®
# -------------------------------
def iter_targets(path: Path) -> Iterable[Tuple[str, str, str]]:
    """
    è®€å– .txt æˆ– .jsonl ç›®æ¨™æ¸…å–®ï¼Œæˆ–è™•ç†æ•´å€‹è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰ .txt æª”æ¡ˆã€‚
    ç”¢å‡º (raw, kind, source_file)ï¼škind = "url" æˆ– "title"ï¼Œsource_file æ˜¯ä¾†æºæª”æ¡ˆè·¯å¾‘
    """
    if path.is_dir():
        # å¦‚æœæ˜¯è³‡æ–™å¤¾ï¼Œæƒæå…¶ä¸­æ‰€æœ‰çš„ .txt æª”æ¡ˆ
        txt_files = list(path.glob("*.txt"))
        jsonl_files = list(path.glob("*.jsonl"))
        all_files = txt_files + jsonl_files
        
        if not all_files:
            print(f"âš ï¸  è³‡æ–™å¤¾ {path} ä¸­æ²’æœ‰æ‰¾åˆ° .txt æˆ– .jsonl æª”æ¡ˆ")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(all_files)} å€‹æª”æ¡ˆï¼š{len(txt_files)} å€‹ .txt æª”æ¡ˆï¼Œ{len(jsonl_files)} å€‹ .jsonl æª”æ¡ˆ")
        
        for file_path in all_files:
            print(f"ğŸ“„ è™•ç†æª”æ¡ˆï¼š{file_path.name}")
            yield from _process_single_file(file_path)
    else:
        # å¦‚æœæ˜¯å–®ä¸€æª”æ¡ˆ
        yield from _process_single_file(path)


def _process_single_file(file_path: Path) -> Iterable[Tuple[str, str, str]]:
    """
    è™•ç†å–®ä¸€æª”æ¡ˆï¼Œç”¢å‡º (raw, kind, source_file)
    """
    source_file = str(file_path)
    
    if file_path.suffix.lower() == ".jsonl":
        with file_path.open("r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    obj = json.loads(line)
                    if "url" in obj and obj["url"]:
                        yield obj["url"], "url", source_file
                    elif "title" in obj and obj["title"]:
                        yield obj["title"], "title", source_file
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  JSON è§£æéŒ¯èª¤åœ¨ {file_path.name}: {e}")
                    continue
    else:
        with file_path.open("r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                s = line.strip()
                if not s or s.startswith("#"):
                    continue
                try:
                    if s.startswith("http://") or s.startswith("https://"):
                        yield s, "url", source_file
                    else:
                        yield s, "title", source_file
                except Exception as e:
                    print(f"âš ï¸  è™•ç†è¡Œ {line_num} æ™‚å‡ºéŒ¯åœ¨ {file_path.name}: {e}")
                    continue


# -------------------------------
# å·¥å…·å‡½æ•¸
# -------------------------------
def url_to_title(url: str) -> str:
    path = urlparse(url).path
    title = unquote(path.rsplit("/", 1)[-1])
    return title

def safe_filename(title: str) -> str:
    name = re.sub(r'[\\/*?:"<>|]', "_", title)
    return name[:200]


def clean_display_title(title_html: str) -> str:
    if not title_html:
        return ""
    return BeautifulSoup(title_html, "html.parser").get_text(" ", strip=True)


def extract_display_title_from_html(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    meta = soup.find("meta", attrs={"property": "mw:displaytitle"})
    if meta and meta.get("content"):
        return meta["content"].strip()
    heading = soup.select_one("#firstHeading")
    if heading:
        return heading.get_text(" ", strip=True)
    title_tag = soup.find("title")
    if title_tag:
        return title_tag.get_text(" ", strip=True)
    return ""

def http_get_with_backoff(session: requests.Session, url: str, *, params=None,
                          timeout: int = 30, retries: int = 3, backoff_base: float = 2.0):
    last_exc: Optional[Exception] = None
    for i in range(retries + 1):
        try:
            r = session.get(url, params=params, timeout=timeout)
            if r.status_code in (429, 503):
                raise RuntimeError(f"HTTP {r.status_code}")
            # Parsoid / API ä¹Ÿå¯èƒ½ä»¥å­—ä¸²æç¤º maxlagï¼Œä¿å®ˆè™•ç†
            if r.status_code == 200 and "maxlag" in r.text.lower() and "error" in r.text.lower():
                raise RuntimeError("Server under high replication lag (maxlag).")
            r.raise_for_status()
            return r
        except Exception as e:
            last_exc = e
            if i == retries:
                break
            time.sleep(backoff_base ** i)  # 2,4,8...
    raise RuntimeError(f"GET failed for {url}: {last_exc}") from last_exc


# -------------------------------
# æŠ“å–ï¼ˆREST å„ªå…ˆï¼ŒAction API å¾Œå‚™ï¼‰
# -------------------------------
def fetch_html_rest(title: str, session: requests.Session, timeout=30) -> tuple[str, str]:
    url = REST_HTML.format(title=quote(title, safe=""))
    r = http_get_with_backoff(session, url, timeout=timeout)
    html = r.text
    display_title = extract_display_title_from_html(html)
    return html, (display_title or title)

def detect_redirect(text: str) -> Optional[str]:
    """
    æª¢æ¸¬é‡å®šå‘é é¢ï¼Œè¿”å›é‡å®šå‘çš„ç›®æ¨™æ¨™é¡Œ
    """
    # æª¢æ¸¬é‡å®šå‘æ¨™è¨˜
    redirect_patterns = [
        r"é‡å®šå‘åˆ°ï¼š\s*â€¢\s*([^\nâ€¢]+)",
        r"é‡æ–°å°å‘è‡³ï¼š\s*â€¢\s*([^\nâ€¢]+)", 
        r"#REDIRECT\s*\[\[([^\]]+)\]\]",
        r"#é‡å®šå‘\s*\[\[([^\]]+)\]\]"
    ]
    
    for pattern in redirect_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            target = match.group(1).strip()
            # æ¸…ç†å¯èƒ½çš„é¡å¤–æ¨™è¨˜
            target = re.sub(r'\|.*$', '', target)  # ç§»é™¤ç®¡é“é€£çµå¾Œçš„éƒ¨åˆ†
            return target
    
    return None


def detect_redirect_from_html(html: str) -> Optional[str]:
    """
    å¾HTMLçµæ§‹æª¢æ¸¬é‡å®šå‘ï¼Œç‰¹åˆ¥æ˜¯è™•ç†REST APIè¿”å›çš„é‡å®šå‘é é¢
    """
    from bs4 import BeautifulSoup
    from urllib.parse import unquote
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºç‰¹æ®Šé‡å®šå‘é é¢
    if 'Special:Redirect' in html:
        soup = BeautifulSoup(html, 'html.parser')
        
        # æª¢æŸ¥ rel="dc:isVersionOf" é€£çµ
        version_link = soup.find('link', rel='dc:isVersionOf')
        if version_link and version_link.get('href'):
            href = version_link['href']
            # æå–é é¢æ¨™é¡Œ (ç§»é™¤ //zh.wikipedia.org/wiki/ å‰ç¶´)
            if '/wiki/' in href:
                encoded_title = href.split('/wiki/')[-1]
                title = unquote(encoded_title)
                return title
        
        # æª¢æŸ¥é é¢æ¨™é¡Œ
        title_tag = soup.find('title')
        if title_tag:
            title_text = title_tag.get_text().strip()
            # ç§»é™¤ " - ç»´åŸºç™¾ç§‘" ç­‰å¾Œç¶´
            title_text = re.sub(r'\s*[-â€“]\s*[^-â€“]*ç»´åŸºç™¾ç§‘[^-â€“]*$', '', title_text)
            if title_text and title_text != 'é‡å®šå‘':
                return title_text
    
    return None


def fetch_html_action(title: str, session: requests.Session, timeout=30) -> Tuple[str, str]:
    """
    æŠ“å–é é¢å…§å®¹ï¼ŒåŒæ™‚è¿”å›å¯¦éš›çš„æ¨™é¡Œï¼ˆè™•ç†é‡å®šå‘ï¼‰
    è¿”å›: (html_content, actual_title)
    """
    params = {
        "action": "parse",
        "page": title,
        "prop": "text|displaytitle",
        "format": "json",
        "variant": "zh-tw",  # æŒ‡å®šè‡ºç£æ­£é«”è®Šé«”
        "maxlag": 5,
    }
    r = http_get_with_backoff(session, API, params=params, timeout=timeout)
    data = r.json()
    if "parse" not in data or "text" not in data["parse"]:
        raise RuntimeError(f"Action parse missing content for: {title}")
    
    # ç²å–å¯¦éš›çš„é é¢æ¨™é¡Œï¼ˆè™•ç†é‡å®šå‘ï¼‰
    display_title = clean_display_title(data["parse"].get("displaytitle"))
    actual_title = display_title or data["parse"].get("title", title)
    html_content = data["parse"]["text"]["*"]
    
    return html_content, actual_title


# -------------------------------
# æ’ç‰ˆæ•´ç†ï¼ˆTidyï¼‰
# -------------------------------
def zh_tidy(text: str) -> str:
    """
    ä¸­è‹±æ··æ’ç©ºç™½èˆ‡æ¨™é»æ•´ç†ï¼ˆä¿ç•™è‹±æ–‡å–®è©å…§ç©ºç™½ï¼›åªä¿®å‰ª CJK èˆ‡æ¨™é»é™„è¿‘ï¼‰ï¼š
    - æ›¸åè™Ÿ/å…§æ›¸åè™Ÿ/å¼•è™Ÿ/æ‹¬è™Ÿã€Œå…§å´ã€å»ç©ºç™½ï¼šã€Š ä½œå“ ã€‹â†’ã€Šä½œå“ã€‹ã€ã€ˆ æ—¥ä¸è½ ã€‰â†’ã€ˆæ—¥ä¸è½ã€‰ã€ï¼ˆ è‹±èªï¼šJolin Tsai ï¼‰â†’ï¼ˆè‹±èªï¼šJolin Tsaiï¼‰
    - æ¨™é»ã€Œå‰ã€å»ç©ºç™½ï¼šâ€¦â€¦ Jolin Tsai ï¼Œâ†’ â€¦â€¦ Jolin Tsaiï¼Œ
    - æ—¥æœŸã€Œå¹´/æœˆ/æ—¥ã€å»ç©ºç™½ï¼š1980å¹´ 9æœˆ 15æ—¥ â†’ 1980å¹´9æœˆ15æ—¥
    - ç ´æŠ˜è™Ÿ/å…©å­—ç ´æŠ˜è™Ÿå‘¨é‚Šå»ç©ºç™½ï¼š â€” â†’ â€”ã€ â€”â€” â†’ â€”â€”
    - CJK èˆ‡ CJK ä¹‹é–“çš„å¤šé¤˜ç©ºç™½å»é™¤ï¼›ã€Œã€ã€å‘¨é‚Šå»ç©ºç™½
    - å£“æ‰ 3 é€£ä»¥ä¸Šç©ºç™½è¡Œ
    """
    # 0) æ¨™æº–åŒ–ä¸å¯è¦‹ç©ºç™½
    text = re.sub(r"[ \t\u00A0]+", " ", text)

    # 0.5) æ¸…é™¤æ•¸å­¸å…¬å¼ LaTeX æ ¼å¼
    # æ¸…é™¤ {\displaystyle ...} æ ¼å¼ï¼ˆåŒ…æ‹¬åµŒå¥—çš„å¤§æ‹¬è™Ÿï¼‰
    text = re.sub(r'\{\\displaystyle[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', '', text)
    # æ¸…é™¤å…¶ä»–å¸¸è¦‹ LaTeX æ ¼å¼
    text = re.sub(r'\{\\[a-zA-Z]+[^}]*\}', '', text)
    # æ¸…é™¤å–®ç¨çš„ {\displaystyle (æ²’æœ‰çµæŸçš„æƒ…æ³)
    text = re.sub(r'\{\\displaystyle.*?(?=\n|$)', '', text)
    # æ¸…é™¤æ›´è¤‡é›œçš„ LaTeX æ•¸å­¸è¡¨é”å¼
    # æ¸…é™¤ \begin{...} ... \end{...} çµæ§‹
    text = re.sub(r'\\begin\{[^}]+\}.*?\\end\{[^}]+\}', '', text, flags=re.DOTALL)
    # æ¸…é™¤ LaTeX å‘½ä»¤
    text = re.sub(r'\\[a-zA-Z]+\{[^}]*\}', '', text)
    text = re.sub(r'\\[a-zA-Z]+', '', text)
    # æ¸…é™¤æ®˜ç•™çš„å¤§æ‹¬è™Ÿå’Œæ•¸å­¸ç¬¦è™Ÿ
    text = re.sub(r'\{[^{}]*\}', '', text)
    # æ¸…é™¤é€£çºŒçš„ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[{}\\^_]+', '', text)

    # 1) æ‹¬è™Ÿ/æ›¸åè™Ÿ/å¼•è™Ÿ å…§å´ç©ºç™½
    pairs = [
        ("ã€Š", "ã€‹"), ("ã€ˆ", "ã€‰"),
        ("ã€Œ", "ã€"), ("ã€", "ã€"),
        ("ï¼ˆ", "ï¼‰")
    ]
    for l, r in pairs:
        text = re.sub(fr"{re.escape(l)}\s*(.*?)\s*{re.escape(r)}", fr"{l}\1{r}", text, flags=re.S)

    # 2) æ¨™é»ã€Œå‰ã€ä¸ç•™ç©ºç™½ï¼ˆä¸­æ–‡æ¨™é»ï¼‰
    text = re.sub(r"\s+([ï¼Œã€‚ã€ï¼›ï¼šï¼ï¼Ÿã€‹ï¼‰ã€ã€])", r"\1", text)

    # 3) æ—¥æœŸã€Œå¹´/æœˆ/æ—¥ã€ä¹‹é–“ä¸ç•™ç©ºç™½
    text = re.sub(r"(\d{1,4})\s*å¹´\s*(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥", r"\1å¹´\2æœˆ\3æ—¥", text)
    text = re.sub(r"(\d{1,4})\s*å¹´\s*(\d{1,2})\s*æœˆ", r"\1å¹´\2æœˆ", text)

    # 4) ç ´æŠ˜è™Ÿå‘¨é‚Šä¸ç•™ç©ºç™½ï¼ˆâ€” èˆ‡ â€”â€”ï¼‰
    text = re.sub(r"\s*â€”â€”\s*", "â€”â€”", text)
    text = re.sub(r"\s*â€”\s*", "â€”", text)

    # 5) CJK èˆ‡ CJK ä¹‹é–“å¤šé¤˜ç©ºç™½å»é™¤ï¼›ã€Œã€ã€å‘¨é‚Šå»ç©ºç™½
    CJK = r"\u4E00-\u9FFF\u3400-\u4DBF"
    # æ™ºèƒ½è™•ç†ï¼šä¿ç•™æ¨™é¡Œé–“çš„æ›è¡Œï¼Œä½†å»é™¤æ®µè½å…§çš„å¤šé¤˜ç©ºç™½
    # å…ˆè™•ç†æ¨™é¡Œè¡Œï¼ˆçŸ­è¡Œä¸”ç¨ç«‹ï¼‰
    lines = text.split('\n')
    for i, line in enumerate(lines):
        stripped = line.strip()
        # å¦‚æœæ˜¯çŸ­è¡Œï¼ˆå¯èƒ½æ˜¯æ¨™é¡Œï¼‰ï¼Œä¿æŒå–®ç¨æˆè¡Œ
        if stripped and len(stripped) < 30 and not any(char in stripped for char in 'ã€‚ï¼Œï¼ï¼Ÿ'):
            continue
        # å°æ–¼é•·æ®µè½ï¼Œå»é™¤ä¸­æ–‡å­—ç¬¦é–“çš„ç©ºæ ¼
        if stripped:
            lines[i] = re.sub(fr"(?<=[{CJK}])[ \t\u00A0]+(?=[{CJK}])", "", stripped)
    
    text = '\n'.join(lines)
    text = re.sub(r"\s*ã€\s*", "ã€", text)

    # 6) é€£çºŒ 3 è¡Œä»¥ä¸Šç©ºç™½ â†’ 2 è¡Œï¼ˆæœ€å¤šåªå…è¨±å…©å€‹æ›è¡Œï¼Œç”¨æ–¼å¤§æ¨™åˆ†éš”ï¼‰
    text = re.sub(r"\n{3,}", "\n\n", text)

    return text


# -------------------------------
# æ–‡å­—æŠ½å–ï¼ˆREST èˆ‡ Action çš†æ”¯æ´ï¼‰
# -------------------------------
def smart_text(node: Tag) -> str:
    """
    é€æ–‡å­—ç¯€é»ä¸²æ¥ï¼š
    - åªæœ‰ã€Œä¸Šä¸€å€‹å­—å…ƒã€èˆ‡ã€Œç•¶å‰ç‰‡æ®µç¬¬ä¸€å€‹å­—å…ƒã€éƒ½ç‚º ASCII å­—æ¯/æ•¸å­—æ™‚ï¼Œæ‰è£œ 1 å€‹ç©ºç™½
    - å…¶ä»–æƒ…æ³ç›´æ¥ç›¸é€£ï¼Œé¿å…åœ¨ CJK é‚Šç•Œè£½é€ å¤šé¤˜ç©ºç™½
    - ç‰¹æ®Šè™•ç†æœ‰åºåˆ—è¡¨ (ol) å’Œç„¡åºåˆ—è¡¨ (ul)
    """
    # å¦‚æœç¯€é»æœ¬èº«æ˜¯åˆ—è¡¨ï¼Œå‰‡ç‰¹æ®Šè™•ç†
    if node.name == "ol":
        return process_ordered_list(node)
    elif node.name == "ul":
        return process_unordered_list(node)
    
    # å…ˆæ”¶é›†æ‰€æœ‰éåˆ—è¡¨å…§å®¹
    text_parts = []
    last_ascii = False
    
    # è™•ç†æ‰€æœ‰ç›´æ¥å­ç¯€é»
    for child in node.children:
        if isinstance(child, NavigableString):
            s = str(child).strip()
            if s:
                cur_ascii = bool(s and s[0].isascii())
                if text_parts and last_ascii and cur_ascii:
                    text_parts.append(" ")
                text_parts.append(s)
                last_ascii = bool(s and s[-1].isascii())
        elif isinstance(child, Tag):
            if child.name in ["ol", "ul"]:
                # è™•ç†åˆ—è¡¨
                if child.name == "ol":
                    list_content = process_ordered_list(child)
                else:
                    list_content = process_unordered_list(child)
                if list_content:
                    text_parts.append(list_content)
                    last_ascii = False
            elif child.name == "br":
                text_parts.append(" ")
                last_ascii = False
            else:
                # éæ­¸è™•ç†å…¶ä»–æ¨™ç±¤
                child_text = smart_text(child)
                if child_text:
                    cur_ascii = bool(child_text and child_text[0].isascii())
                    if text_parts and last_ascii and cur_ascii:
                        text_parts.append(" ")
                    text_parts.append(child_text)
                    last_ascii = bool(child_text and child_text[-1].isascii())
    
    return "".join(text_parts)


def process_ordered_list(ol_tag: Tag) -> str:
    """è™•ç†æœ‰åºåˆ—è¡¨ï¼Œæ·»åŠ åºè™Ÿ"""
    items = []
    lis = ol_tag.find_all("li", recursive=False)
    for i, li in enumerate(lis, 1):
        if isinstance(li, Tag):
            li_text = extract_text_from_li(li)
            if li_text:
                items.append(f"{i}. {li_text}")
    return " ".join(items)


def process_unordered_list(ul_tag: Tag) -> str:
    """è™•ç†ç„¡åºåˆ—è¡¨ï¼Œæ·»åŠ é …ç›®ç¬¦è™Ÿ"""
    items = []
    for li in ul_tag.find_all("li", recursive=False):
        li_text = extract_text_from_li(li)
        if li_text:
            items.append(f"â€¢ {li_text}")
    return " ".join(items)


def extract_text_from_li(li_tag: Tag) -> str:
    """å¾ li æ¨™ç±¤ä¸­æå–æ–‡å­—ï¼Œé¿å…éæ­¸è™•ç†åµŒå¥—åˆ—è¡¨"""
    parts = []
    last_ascii = False
    
    for d in li_tag.descendants:
        if isinstance(d, NavigableString):
            # è·³éåµŒå¥—åˆ—è¡¨ä¸­çš„æ–‡å­—
            parent_li = d.find_parent("li")
            if parent_li and parent_li != li_tag:
                continue
                
            s = str(d)
            if not s or not s.strip():
                continue
            s = s.strip()
            cur_ascii = bool(s and s[0].isascii())
            if parts and last_ascii and cur_ascii:
                parts.append(" ")
            parts.append(s)
            last_ascii = bool(s and s[-1].isascii())
        elif isinstance(d, Tag) and d.name == "br":
            parts.append(" ")
            last_ascii = False
    
    return "".join(parts)


def _parse_int(value, default: int = 1) -> int:
    try:
        return max(int(value), 1)
    except (TypeError, ValueError):
        return default


def table_to_lines(table: Tag, squeeze) -> list[str]:
    """Flatten a wiki table into pipe-separated rows with colspan/rowspan expansion."""
    lines: list[str] = []

    caption = table.find("caption")
    if caption:
        caption_text = squeeze(smart_text(caption))
        if caption_text:
            lines.append(caption_text)

    def table_squeeze(s: str) -> str:
        """è¡¨æ ¼å°ˆç”¨çš„æ–‡æœ¬å£“ç¸®ï¼Œä¿ç•™æ›è¡Œç¬¦ä½†è½‰æ›ç‚ºç©ºæ ¼"""
        # å°‡æ›è¡Œç¬¦æ›¿æ›ç‚ºç©ºæ ¼ï¼Œä¿æŒåœ¨åŒä¸€å€‹å–®å…ƒæ ¼å…§
        s = s.replace('\n', ' ')
        # ç„¶å¾Œé€²è¡Œæ­£å¸¸çš„ç©ºç™½å£“ç¸®
        return re.sub(r"[ \t\u00A0]+", " ", s.strip())

    row_spans: list[dict | None] = []
    rows: list[list[str]] = []

    for tr in table.find_all("tr"):
        if tr.find_parent("table") is not table:
            continue

        current_row: list[str | None] = []

        # apply active rowspans to this row
        for idx in range(len(row_spans)):
            span = row_spans[idx]
            if span:
                current_row.append(span["value"])
                span["rows_left"] -= 1
                if span["rows_left"] == 0:
                    row_spans[idx] = None
            else:
                current_row.append(None)

        cells = tr.find_all(["th", "td"], recursive=False)
        for cell in cells:
            # èª¿è©¦ï¼šæª¢æŸ¥æ˜¯å¦æœ‰ ol æ¨™ç±¤
            if isinstance(cell, Tag) and cell.find("ol"):
                ol_tags = cell.find_all("ol")
                for i, ol in enumerate(ol_tags):
                    if isinstance(ol, Tag):
                        processed_ol = process_ordered_list(ol)
            
            if isinstance(cell, Tag):
                text = table_squeeze(smart_text(cell))
                colspan = _parse_int(cell.get("colspan"), 1)
                rowspan = _parse_int(cell.get("rowspan"), 1)
            else:
                continue

            # find the first slot that can host this cell (respecting existing rowspans)
            col_idx = 0
            while True:
                # extend row/rowspan buffers when needed
                while col_idx >= len(current_row):
                    current_row.append(None)
                    row_spans.append(None)

                # ensure the span fits starting at col_idx
                fits = True
                for offset in range(colspan):
                    pos = col_idx + offset
                    while pos >= len(current_row):
                        current_row.append(None)
                        row_spans.append(None)
                    if current_row[pos] is not None:
                        col_idx = pos + 1
                        fits = False
                        break
                if fits:
                    break

            # place the cell content across the colspan
            for offset in range(colspan):
                pos = col_idx + offset
                value = text if offset == 0 else ""
                current_row[pos] = value
                if rowspan > 1:
                    row_spans[pos] = {"rows_left": rowspan - 1, "value": value}
                else:
                    if row_spans[pos] is None:
                        continue
                    row_spans[pos] = None

        cleaned = [c if c is not None else "" for c in current_row]
        while cleaned and not cleaned[-1].strip():
            cleaned.pop()
        if any(col.strip() for col in cleaned):
            rows.append(cleaned)

    for row in rows:
        line = " | ".join(row)
        if line.strip():
            lines.append("â€¢ " + line)

    return lines


def process_multicol_table(table: Tag, squeeze) -> list[str]:
    """
    è™•ç† multicol è¡¨æ ¼ï¼Œé€™ç¨®è¡¨æ ¼é€šå¸¸åŒ…å«å¤šæ¬„å…§å®¹ï¼Œæ¯æ¬„æœ‰æ¨™é¡Œå’Œåˆ—è¡¨
    ä¾‹å¦‚ï¼šéŸ³æ¨‚ä½œå“æ¬„ä½ï¼ŒåŒ…å« "éŒ„éŸ³å®¤å°ˆè¼¯"ã€"ç²¾é¸è¼¯"ã€"æ—¥èªä½œå“" ç­‰åˆ†é¡
    """
    lines: list[str] = []
    
    # æŸ¥æ‰¾æ‰€æœ‰çš„ td æ¬„ä½ï¼ŒæŒ‰é †åºè™•ç†æ¯å€‹æ¬„ä½
    for td in table.find_all("td"):
        # æŒ‰æ–‡æª”é †åºè™•ç†è©²æ¬„ä½ä¸­çš„æ‰€æœ‰å­å…ƒç´ 
        for element in td.find_all(["h3", "h4", "h5", "h6", "dl", "ul", "ol"], recursive=True):
            
            # è™•ç†æ¨™é¡Œï¼ˆh3, h4 ç­‰ï¼‰
            if element.name in ["h3", "h4", "h5", "h6"]:
                title = squeeze(smart_text(element))
                if title:
                    # æ·»åŠ å­æ¨™é¡Œï¼ˆå‰é¢æœ‰ç©ºè¡Œåˆ†éš”ï¼‰
                    if lines and lines[-1] != "":
                        lines.append("")
                    lines.append(title)
            
            # è™•ç† dl æ¨™é¡Œ
            elif element.name == "dl":
                for dt in element.find_all("dt"):
                    title = squeeze(smart_text(dt))
                    if title:
                        # æ·»åŠ å­æ¨™é¡Œï¼ˆå‰é¢æœ‰ç©ºè¡Œåˆ†éš”ï¼‰
                        if lines and lines[-1] != "":
                            lines.append("")
                        lines.append(f"### {title}")
            
            # è™•ç†ç„¡åºåˆ—è¡¨ ul
            elif element.name == "ul":
                # åªè™•ç†ç›´æ¥å­é …ç›®ï¼Œé¿å…é‡è¤‡è™•ç†åµŒå¥—åˆ—è¡¨
                for li in element.find_all("li", recursive=False):
                    item = squeeze(smart_text(li))
                    if item:
                        lines.append(f"â€¢ {item}")
            
            # è™•ç†æœ‰åºåˆ—è¡¨ ol
            elif element.name == "ol":
                # åªè™•ç†ç›´æ¥å­é …ç›®ï¼Œé¿å…é‡è¤‡è™•ç†åµŒå¥—åˆ—è¡¨
                for li in element.find_all("li", recursive=False):
                    item = squeeze(smart_text(li))
                    if item:
                        lines.append(f"â€¢ {item}")
    
    return lines


# ===== é€šç”¨æ¨™ç±¤è™•ç†ï¼ˆå­¸å / è—å / æœ¬å / åŸå / åˆ¥å / å¤šèªè¨€ï¼‰ =====

# å¸¸è¦‹æ¨™ç±¤ â†’ BCP-47 èªè¨€ç¢¼ï¼ˆå¯è‡ªè¡Œæ“´å……ï¼‰
LABEL_LANG_MAP = {
    # èªè¨€åç¨±ï¼ˆå«å¸¸è¦‹åˆ¥ç¨±ï¼‰
    "è‹±èª": "en", "è‹±æ–‡": "en", "English": "en",
    "æ—¥èª": "ja", "æ—¥æ–‡": "ja", "Japanese": "ja",
    "éŸ“èª": "ko", "éŸ“æ–‡": "ko",
    "æ³•èª": "fr", "æ³•æ–‡": "fr",
    "å¾·èª": "de", "å¾·æ–‡": "de",
    "è¥¿ç­ç‰™èª": "es", "è¥¿æ–‡": "es", "è¥¿èª": "es",
    "ä¿„èª": "ru", "ä¿„æ–‡": "ru",
    "ç¾©å¤§åˆ©èª": "it", "ç¾©æ–‡": "it", "æ„å¤§åˆ©èª": "it", "æ„æ–‡": "it",
    "è‘¡è„ç‰™èª": "pt", "è‘¡æ–‡": "pt",
    "æ‹‰ä¸èª": "la", "æ‹‰ä¸æ–‡": "la",
    "è¶Šå—èª": "vi", "è¶Šæ–‡": "vi",
    "æ³°èª": "th",
    "é¦¬ä¾†èª": "ms", "é¦¬ä¾†æ–‡": "ms",
    "å°å°¼èª": "id", "å°å°¼æ–‡": "id",
    "ç²µèª": "yue", "å»£æ±è©±": "yue",
    "é–©å—èª": "nan", "è‡ºèª": "nan", "å°èª": "nan", "é–©å—è©±": "nan",
    "å®¢èª": "hak",
    # éèªè¨€ä½†å¸¸è¦‹ï¼šå­¸åï¼ˆå¤šåŠæ‹‰ä¸ï¼‰
    "å­¸å": "la",
}

# æœƒè¢«ç•¶æˆã€Œå€¼ç¼ºå¤±ã€çš„åˆ†éš”ç¬¦ï¼ˆé‡åˆ°å®ƒå€‘å°±åœæ­¢è’é›†ï¼‰
_STOP_CHARS = "ï¼Œ,ã€ï¼›;,)ï¼‰ã€ã€ã€‘ã€‰ã€‚."

def _fetch_langlink_title(title: str, session: requests.Session, lang_code: str) -> Optional[str]:
    """
    ç”¨ MediaWiki Action API è®€å–äº’èªé€£çµæ¨™é¡Œï¼ˆlllang=<code>ï¼‰
    """
    params = {
        "action": "query",
        "prop": "langlinks",
        "titles": title,
        "lllang": lang_code,
        "format": "json",
    }
    try:
        r = http_get_with_backoff(session, API, params=params, timeout=20)
        data = r.json()
        pages = data.get("query", {}).get("pages", {})
        for _, page in pages.items():
            for ll in page.get("langlinks", []) or []:
                if ll.get("lang") == lang_code:
                    return ll.get("*") or ll.get("title")
    except Exception:
        pass
    return None


def _collect_value_after_label(el: Tag, label: str, maxlen: int = 120) -> Optional[str]:
    """
    åœ¨æ®µè½ el è£¡æ‰¾åˆ°ã€Œ<label>ï¼šã€é€™å€‹é‚è¼¯åºåˆ—ï¼ˆå…è¨± label èˆ‡å†’è™Ÿè·¨ç¯€é»ï¼‰ï¼Œ
    ä¹‹å¾ŒæŠŠå€¼ä¸€è·¯æ”¶é›†åˆ°é‡åˆ°åœæ­¢å­—å…ƒç‚ºæ­¢ã€‚
    ä¾‹ï¼š<a>å­¸å</a>ï¼š<i><span lang="la">Oncorhynchus masou formosanus</span></i>
    """
    # 1) æ‰¾åˆ°æ–‡å­—å‰›å¥½ç­‰æ–¼ label çš„ç¯€é»ï¼ˆå¯èƒ½æ˜¯ NavigableString æˆ– <a>/<b>/<span> ç­‰ Tagï¼‰
    anchor: Optional[object] = None
    for d in el.descendants:
        if isinstance(d, NavigableString):
            if d.strip() == label:
                anchor = d
                break
        elif isinstance(d, Tag):
            # åƒ…å–ç´”æ–‡å­—å‰›å¥½ç­‰æ–¼ label çš„ç¯€é»ï¼Œé¿å…æŠ“åˆ°é•·å¥å­ä¸­çš„ç‰‡æ®µ
            if d.get_text("", strip=True) == label:
                anchor = d
                break
    if not anchor:
        return None

    # 2) å¾ anchor å¾€å¾Œæ‰¾ã€Œç¬¬ä¸€å€‹å†’è™Ÿã€ï¼ˆå…è¨±å…¨/åŠå½¢ï¼‰ï¼Œåƒæ‰å®ƒä¹‹å¾Œé–‹å§‹æ”¶é›†å€¼
    it = anchor.next_elements
    saw_colon = False
    parts: list[str] = []

    def cut_at_stop(s: str) -> tuple[str, bool]:
        """æŠŠ s ç åˆ°ç¬¬ä¸€å€‹åœæ­¢å­—å…ƒï¼Œå›å‚³ (æœ‰æ•ˆå…§å®¹, æ˜¯å¦é‡åˆ°åœæ­¢)"""
        s = s.strip()
        if not s:
            return "", False
        for idx, ch in enumerate(s):
            if ch in _STOP_CHARS:
                return s[:idx], True
        return s, False

    # 3) å¯èƒ½å­˜åœ¨é€™ç¨®æƒ…æ³ï¼šlabel çš„**ä¸‹ä¸€å€‹**æ–‡å­—ç¯€é»ä¸€é–‹å§‹å°±æ˜¯ã€Œï¼šã€æˆ–å«ã€Œï¼š...ã€
    for node in it:
        if isinstance(node, NavigableString):
            s = str(node)
            if not saw_colon:
                pos = s.find("ï¼š")
                if pos < 0:
                    pos = s.find(":")  # ä¹Ÿå®¹è¨±åŠå½¢å†’è™Ÿ
                if pos < 0:
                    # é‚„æ²’çœ‹åˆ°å†’è™Ÿï¼Œç•¥é
                    continue
                # åƒæ‰å†’è™Ÿï¼Œå†’è™Ÿå¾Œå¯èƒ½ç•¶å ´å°±æœ‰å€¼
                s = s[pos + 1:]
                saw_colon = True

            # å·²çœ‹åˆ°å†’è™Ÿï¼Œé–‹å§‹æ”¶é›†å€¼ï¼Œç›´åˆ°é‡åˆ°åœæ­¢å­—å…ƒ
            chunk, stop = cut_at_stop(s)
            if chunk:
                parts.append(chunk)
            if stop:
                break

        elif isinstance(node, Tag):
            # Tag å…§çš„æ–‡å­—æœƒåœ¨å…¶ NavigableString å­å­«ç¯€é»è¢«è™•ç†ï¼›
            # ä½†è‹¥æ˜¯ <br>ï¼Œè¦–ç‚ºç¡¬æ›è¡Œä¸æ”¶é›†
            if node.name == "br":
                break

        # å®‰å…¨ä¸Šé™ï¼Œé¿å…ç•°å¸¸é•·åº¦
        if sum(len(p) for p in parts) > maxlen * 2:
            break

    val = " ".join(p for p in parts if p).strip()
    return val[:maxlen] if val else None

def _collect_value_after_marker(el: Tag, marker_text: str, maxlen: int = 120) -> Optional[str]:
    """
    å¾å«æœ‰ marker_textï¼ˆå¦‚ã€Œå­¸åï¼šã€ã€Œè‹±èªï¼šã€ï¼‰çš„æ®µè½ el å…§ï¼Œ
    æ²¿ DOM next_elements é€£çºŒè’é›†æ–‡å­—ï¼Œç›´åˆ°é‡åˆ° _STOP_CHARSã€‚
    """
    needle = None
    for d in el.descendants:
        if isinstance(d, NavigableString):
            s = str(d)
            if marker_text in s:
                needle = d
                break
    if not needle:
        return None

    parts: list[str] = []
    tail = str(needle).split(marker_text, 1)[1]

    def consume(s: str) -> tuple[str, bool]:
        s = s.strip()
        if not s:
            return "", False
        indices = [i for i, ch in enumerate(s) if ch in _STOP_CHARS]
        if indices:
            i = min(indices)
            return s[:i], True
        return s, False

    if tail:
        chunk, stop = consume(tail)
        if chunk:
            parts.append(chunk)
        if stop:
            out = " ".join(p for p in parts if p).strip()
            return out[:maxlen] if out else None

    for node in needle.next_elements:
        if isinstance(node, NavigableString):
            chunk, stop = consume(str(node))
            if chunk:
                parts.append(chunk)
            if stop:
                break
        elif isinstance(node, Tag):
            classes = " ".join(node.get("class", [])).lower()
            if "reference" in classes or "mw-editsection" in classes:
                continue
        if sum(len(p) for p in parts) > maxlen * 1.5:
            break

    out = " ".join(p for p in parts if p).strip(" ")
    return out[:maxlen] if out else None


# æ¨™ç±¤æ ¸å¿ƒé›†åˆï¼ˆèªè¨€ + å¸¸è¦‹åˆ¥åé¡æ¨™ç±¤ï¼‰
_LABEL_CORE_RE = r"(è‹±èª|è‹±æ–‡|English|æ—¥èª|æ—¥æ–‡|Japanese|éŸ“èª|éŸ“æ–‡|æ³•èª|æ³•æ–‡|å¾·èª|å¾·æ–‡|è¥¿ç­ç‰™èª|è¥¿æ–‡|è¥¿èª|ä¿„èª|ä¿„æ–‡|ç¾©å¤§åˆ©èª|ç¾©æ–‡|æ„å¤§åˆ©èª|æ„æ–‡|è‘¡è„ç‰™èª|è‘¡æ–‡|æ‹‰ä¸èª|æ‹‰ä¸æ–‡|è¶Šå—èª|è¶Šæ–‡|æ³°èª|é¦¬ä¾†èª|é¦¬ä¾†æ–‡|å°å°¼èª|å°å°¼æ–‡|ç²µèª|å»£æ±è©±|é–©å—èª|è‡ºèª|å°èª|é–©å—è©±|å®¢èª|å­¸å|è—å|æœ¬å|åŸå|èˆŠç¨±|åˆå|åˆ¥å|åˆ¥ç¨±|å¤–æ–‡)"

# åµæ¸¬ã€Œæ¨™ç±¤ï¼šã€ä¹‹å¾Œæ¥åˆ†éš”ç¬¦ï¼ˆä»£è¡¨ç¼ºå€¼ï¼‰
_MISSING_AFTER_LABEL = re.compile(_LABEL_CORE_RE + r"ï¼š(?=\s*[ï¼Œã€ï¼›)ï¼‰])")

def fill_labels_if_missing(el: Tag, txt: str) -> str:
    """
    è‹¥åµæ¸¬åˆ°ã€Œ<æ¨™ç±¤>ï¼šã€å¾Œç·Šè·Ÿåˆ†éš”ç¬¦ï¼ˆå€¼ç¼ºå¤±ï¼‰ï¼Œå…ˆç”¨ã€Œæ¨™ç±¤åˆ†ç¯€é»ã€æ³•è£œï¼Œ
    è‹¥å¤±æ•—å†ç”¨ã€Œæ•´æ®µ markerã€æ³•è£œã€‚
    """
    def _repl(m: re.Match) -> str:
        label = m.group(1)
        # å…ˆç”¨åŸæ–¹æ³•æ‰¾
        val = _collect_value_after_label(el, label)
        if not val:
            # åŒä¸€ text node çš„æ¨™ç±¤ï¼šå†’è™Ÿæƒ…æ³
            val = _collect_value_after_marker(el, f"{label}ï¼š")
        return f"{label}ï¼š{val}" if val else m.group(0)

    return _MISSING_AFTER_LABEL.sub(_repl, txt)

def ensure_labels_after_marker(text: str, *, title: str, session: requests.Session) -> str:
    """
    å…¨æ–‡å±¤ç´šæœ€çµ‚å…œåº•ï¼šå°ä»ã€Œ<èªè¨€æ¨™ç±¤>ï¼š<ç¼ºå€¼>ã€è€…ï¼Œå˜—è©¦ç”¨ langlinks å–å°æ‡‰èªè¨€æ¨™é¡Œè£œä¸Šã€‚
    åƒ…å° LABEL_LANG_MAP æœ‰å°æ‡‰ç¢¼çš„æ¨™ç±¤ç”Ÿæ•ˆï¼›å…¶ä»–ï¼ˆå¦‚ è—å/æœ¬åï¼‰ä¸åœ¨æ­¤è™•è£œã€‚
    """
    def _repl(m: re.Match) -> str:
        label = m.group(1)
        code = LABEL_LANG_MAP.get(label)
        if not code:
            return m.group(0)
        name = _fetch_langlink_title(title, session, code)
        return f"{label}ï¼š{name}" if name else m.group(0)

    return _MISSING_AFTER_LABEL.sub(_repl, text)


def extract_specific_caption_from_tmulti(img, description_text, tmulti_container):
    """
    å¾ tmulti å®¹å™¨çš„ç¸½é«”æè¿°ä¸­æå–ç‰¹å®šåœ–ç‰‡çš„èªªæ˜
    """
    try:
        # æª¢æŸ¥è‹—æ —å¸‚çš„æ ¼å¼ï¼š"ä¸Šï¼š...ä¸­ï¼š...ä¸‹ï¼š..."
        if "ä¸Šï¼š" in description_text and "ä¸­ï¼š" in description_text and "ä¸‹ï¼š" in description_text:
            # è§£æä¸Šä¸­ä¸‹æ ¼å¼
            sections = {}
            
            # æå–ä¸Šéƒ¨åˆ†
            if "ä¸Šï¼š" in description_text:
                start = description_text.find("ä¸Šï¼š") + 2
                end = description_text.find("ä¸­ï¼š")
                if end != -1:
                    sections["ä¸Š"] = description_text[start:end].strip()
            
            # æå–ä¸­éƒ¨åˆ†
            if "ä¸­ï¼š" in description_text:
                start = description_text.find("ä¸­ï¼š") + 2
                end = description_text.find("ä¸‹ï¼š")
                if end != -1:
                    sections["ä¸­"] = description_text[start:end].strip()
            
            # æå–ä¸‹éƒ¨åˆ†
            if "ä¸‹ï¼š" in description_text:
                start = description_text.find("ä¸‹ï¼š") + 2
                sections["ä¸‹"] = description_text[start:].strip()
            
            # æ ¹æ“šåœ–ç‰‡ä½ç½®è¿”å›å°æ‡‰èªªæ˜
            img_position = get_image_position_in_tmulti(img, tmulti_container)
            print(f"è‹—æ —å¸‚åœ–ç‰‡ä½ç½®: {img_position}")
            
            if img_position == 0 and "ä¸Š" in sections:
                return sections["ä¸Š"]
            elif img_position in [1, 2] and "ä¸­" in sections:
                # ä¸­é–“éƒ¨åˆ†å¯èƒ½æœ‰å¤šå€‹é …ç›®ï¼Œåˆ†å‰²è™•ç†
                middle_text = sections["ä¸­"]
                middle_items = [item.strip() for item in middle_text.split("ã€")]
                if len(middle_items) > 1:
                    if img_position == 1:
                        return middle_items[0]  # è‹—æ —å·¨è›‹
                    elif img_position == 2:
                        return middle_items[1]  # è‹—æ —å¸‚å¤©åå®®
                return middle_text
            elif img_position is not None and img_position >= 3 and "ä¸‹" in sections:
                # ä¸‹é¢éƒ¨åˆ†å¯èƒ½æœ‰å¤šå€‹é …ç›®
                bottom_text = sections["ä¸‹"]
                bottom_items = [item.strip() for item in bottom_text.split("ã€")]
                bottom_position = img_position - 3
                if bottom_position < len(bottom_items):
                    return bottom_items[bottom_position]
                return bottom_text
        
        # æª¢æŸ¥é«˜é›„å¸‚çš„æ ¼å¼ï¼š"ç”±å·¦è‡³å³ã€å¾ä¸Šè‡³ä¸‹ï¼š..."
        elif "ç”±å·¦è‡³å³" in description_text and "ï¼š" in description_text:
            # è§£æ "ç”±å·¦è‡³å³ã€å¾ä¸Šè‡³ä¸‹ï¼šé«˜é›„å¸‚å€å¤œæ™¯ã€é³³å±±ç¸£èˆŠåŸé³³å„€é–€ã€ç‰å±±ä¸»å³°ã€è¡›æ­¦ç‡Ÿåœ‹å®¶è—è¡“æ–‡åŒ–ä¸­å¿ƒ..."
            parts = description_text.split("ï¼š", 1)
            if len(parts) == 2:
                items = [item.strip() for item in parts[1].split("ã€")]
                
                # å˜—è©¦ç¢ºå®šåœ–ç‰‡åœ¨å®¹å™¨ä¸­çš„ä½ç½®
                img_position = get_image_position_in_tmulti(img, tmulti_container)
                if img_position is not None and 0 <= img_position < len(items):
                    return items[img_position]
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šé—œéµè©ï¼Œå¦‚è¡›æ­¦ç‡Ÿ
        img_src = str(img.get("src", "")).lower()
        if "wei-wu-ying" in img_src and "è¡›æ­¦ç‡Ÿ" in description_text:
            return "è¡›æ­¦ç‡Ÿåœ‹å®¶è—è¡“æ–‡åŒ–ä¸­å¿ƒ"
        elif "kaohsiung_skyline" in img_src and ("å¤œæ™¯" in description_text or "å¸‚å€" in description_text):
            return "é«˜é›„å¸‚å€å¤œæ™¯"
        elif "é³³å±±" in description_text and ("é³³å„€é–€" in description_text or "èˆŠåŸ" in description_text):
            return "é³³å±±ç¸£èˆŠåŸé³³å„€é–€"
        elif "ç‰å±±" in description_text and "ä¸»å³°" in description_text:
            return "ç‰å±±ä¸»å³°"
        elif "é¾è™å¡”" in description_text:
            return "é¾è™å¡”"
        elif "å…‰ä¹‹ç©¹é ‚" in description_text:
            return "å…‰ä¹‹ç©¹é ‚"
        elif "é«”è‚²å ´" in description_text or "stadium" in img_src.lower():
            return "åœ‹å®¶é«”è‚²å ´"
        
        # è‹—æ —å¸‚ç‰¹å®šé—œéµè©åŒ¹é…
        elif "miaoli_station" in img_src or "tra_miaoli" in img_src:
            return "è‡ºéµè‹—æ —è»Šç«™"
        elif "miaoli_arena" in img_src:
            return "è‹—æ —å·¨è›‹"
        elif "tianhou" in img_src or "å¤©åå®®" in description_text:
            return "è‹—æ —å¸‚å¤©åå®®"
        elif "wenchang" in img_src or "æ–‡æ˜Œç¥ " in description_text:
            return "è‹—æ —æ–‡æ˜Œç¥ "
        elif "miaoli" in img_src and ("è¡—æ™¯" in description_text or "panoramio" in img_src):
            return "è‹—æ —è¡—æ™¯"
        
    except Exception as e:
        print(f"è§£æ tmulti caption æ™‚å‡ºéŒ¯: {e}")
    
    return ""

def get_image_position_in_tmulti(img, tmulti_container):
    """
    ç²å–åœ–ç‰‡åœ¨ tmulti å®¹å™¨ä¸­çš„ä½ç½®ï¼ˆå¾0é–‹å§‹ï¼‰
    """
    try:
        # æ‰¾åˆ°æ‰€æœ‰çš„ tsingle å…ƒç´ 
        tsingle_elements = tmulti_container.find_all("div", class_="tsingle")
        
        # æ‰¾åˆ°åŒ…å«è©²åœ–ç‰‡çš„ tsingle
        img_tsingle = img.find_parent("div", class_="tsingle")
        if img_tsingle and img_tsingle in tsingle_elements:
            return tsingle_elements.index(img_tsingle)
    except Exception:
        pass
    
    return None

def download_image(url: str, output_dir: Path, session: requests.Session) -> Optional[str]:
    """
    ä¸‹è¼‰åœ–ç‰‡ä¸¦è¿”å›æœ¬åœ°æ–‡ä»¶å
    """
    try:
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
        url_hash = hashlib.md5(url.encode()).hexdigest()[:12]
        
        # å¾URLç²å–æ–‡ä»¶æ“´å±•å
        parsed_url = urlparse(url)
        path = parsed_url.path.lower()
        if path.endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg')):
            ext = path.split('.')[-1]
        else:
            ext = 'jpg'  # é»˜èªç‚ºjpg
        
        filename = f"{url_hash}.{ext}"
        filepath = output_dir / filename
        
        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if filepath.exists():
            return filename
        
        # ä¸‹è¼‰åœ–ç‰‡
        response = session.get(url, timeout=30)
        response.raise_for_status()
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶
        with open(filepath, 'wb') as f:
            f.write(response.content)
        
        print(f"âœ… ä¸‹è¼‰åœ–ç‰‡: {filename}")
        return filename
        
    except Exception as e:
        print(f"âŒ åœ–ç‰‡ä¸‹è¼‰å¤±æ•— {url}: {e}")
        return None


def extract_images_from_infoboxes(soup: BeautifulSoup, title: str, source_url: str, 
                                 images_dir: Path, session: requests.Session) -> Tuple[list[str], list[dict]]:
    """
    å¾è³‡è¨Šæ¡†ï¼ˆinfoboxï¼‰ä¸­æå–ä¸»è¦åœ–ç‰‡è³‡è¨Šï¼Œä¸‹è¼‰åœ–ç‰‡ä¸¦è¿”å›åœ–ç‰‡è³‡è¨Š
    è¿”å›: (æ–‡æœ¬ä¸­çš„åœ–ç‰‡æè¿°åˆ—è¡¨, JSONLæ ¼å¼çš„åœ–ç‰‡è³‡è¨Šåˆ—è¡¨)
    """
    image_info_text = []
    image_info_json = []
    
    # æŸ¥æ‰¾è³‡è¨Šæ¡†ä¸­çš„ä¸»è¦åœ–ç‰‡ï¼ˆé€šå¸¸æ˜¯äººç‰©ç…§ç‰‡ç­‰ï¼‰
    infoboxes = soup.select("table.infobox")
    
    for infobox in infoboxes:
        # æŸ¥æ‰¾è³‡è¨Šæ¡†ä¸­çš„åœ–ç‰‡å€åŸŸ - æ“´å±•æ”¯æ´æ›´å¤šé¡åˆ¥
        image_selectors = [
            "td.infobox-image",        # æ¨™æº–äººç‰©ç…§ç‰‡
            ".infobox-image",          # ä¸€èˆ¬ infobox åœ–ç‰‡
            ".ib-settlement-cols-cell", # åŸå¸‚/åœ°å€çš„åœ–ç‰‡ï¼ˆå¦‚å¸‚å¾½ï¼‰
            "td.maptable",             # åœ°åœ–è¡¨æ ¼
            ".infobox-full-data",      # å®Œæ•´è³‡æ–™å€åŸŸ
            ".tmulti",                 # å¤šåœ–ç‰‡å®¹å™¨ï¼ˆå¦‚åŸå¸‚æ™¯è§€ç…§ç‰‡ï¼‰
            ".thumb",                  # ç¸®ç•¥åœ–å®¹å™¨
            ".tsingle"                 # å–®åœ–ç‰‡å®¹å™¨
        ]
        
        image_cells = []
        for selector in image_selectors:
            image_cells.extend(infobox.select(selector))
        
        for cell in image_cells:
            # æŸ¥æ‰¾åœ–ç‰‡å…ƒç´ 
            img = cell.find("img")
            if not img:
                continue
                
            # ç²å–åœ–ç‰‡URL
            src = img.get("src") or img.get("data-src")
            if not src:
                continue
            
            # è½‰æ›ç‚ºå­—ç¬¦ä¸²ä¸¦è™•ç†URL
            src = str(src)
            if src.startswith("//"):
                src = "https:" + src
            elif src.startswith("/"):
                src = "https://zh.wikipedia.org" + src
            
            # éæ¿¾æ‰å°åœ–ç¤ºã€ç·¨è¼¯åœ–æ¨™ã€æ”¿é»¨æ¨™èªŒã€SVG åœ–ç‰‡å’Œå—é™åˆ¶çš„åœ°åœ–åœ–ç‰‡
            filter_keywords = [
                "edit", "icon", "20px", "commons/thumb/8/8a/ooj",
                "emblem_of_the_kuomintang",  # åœ‹æ°‘é»¨åœ–æ¨™
                "independent_candidate_icon", # ç„¡é»¨ç±åœ–æ¨™ 
                "disambig_gray",             # æ¶ˆæ­§ç¾©åœ–æ¨™
                "information_icon4",         # è³‡è¨Šåœ–æ¨™
                "40px-", "60px-",            # å°å°ºå¯¸åœ–ç‰‡
                "chinese_characters",        # ä¸­æ–‡å­—é«”åœ–ç‰‡
                "characters",                # å­—é«”åœ–ç‰‡
                "phonetic",                  # éŸ³æ¨™åœ–ç‰‡
                "template",                  # æ¨¡æ¿åœ–ç‰‡
                "maps.wikimedia.org",        # Wikimedia åœ°åœ–ç“¦ç‰‡ (å—é™åˆ¶)
                "osm-intl",                  # OpenStreetMap åœ°åœ–ç“¦ç‰‡
                "maplink",                   # åœ°åœ–é€£çµåœ–ç‰‡
                "mapframe"                   # åœ°åœ–æ¡†æ¶åœ–ç‰‡
            ]
            
            # éæ¿¾æ‰ SVG åœ–ç‰‡
            if src.lower().endswith('.svg') or '.svg/' in src.lower():
                continue
            
            if any(keyword in src.lower() for keyword in filter_keywords):
                continue
            
            # éæ¿¾æ‰åœ–ç‰‡å°ºå¯¸å¤ªå°çš„ï¼ˆå¯¬åº¦æˆ–é«˜åº¦å°æ–¼80pxï¼‰
            try:
                width = img.get("width")
                height = img.get("height")
                if width and str(width).replace("px", "").isdigit() and int(str(width).replace("px", "")) < 80:
                    continue
                if height and str(height).replace("px", "").isdigit() and int(str(height).replace("px", "")) < 80:
                    continue
                    
                # ç‰¹åˆ¥é‡å°æ–‡å­—åœ–ç‰‡çš„éæ¿¾ï¼šå¦‚æœå¯¬åº¦å¤§æ–¼é«˜åº¦çš„2å€ï¼ˆå¯èƒ½æ˜¯æ–‡å­—åœ–ç‰‡ï¼‰
                if width and height:
                    try:
                        w = int(str(width).replace("px", ""))
                        h = int(str(height).replace("px", ""))
                        if w > 0 and h > 0:
                            ratio = w / h
                            # å¦‚æœå¯¬é«˜æ¯”å¤§æ–¼3:1ï¼Œå¾ˆå¯èƒ½æ˜¯æ–‡å­—åœ–ç‰‡
                            if ratio > 3 and w < 300:  # å¯¬åº¦å°æ–¼300px ä¸”å¯¬é«˜æ¯”å¤§æ–¼3:1
                                print(f"â­ è·³éæ–‡å­—åœ–ç‰‡: {src} (å°ºå¯¸: {w}x{h}, æ¯”ä¾‹: {ratio:.2f})")
                                continue
                    except (ValueError, TypeError):
                        pass
            except (ValueError, AttributeError):
                pass  # å¦‚æœç„¡æ³•è§£æå°ºå¯¸ï¼Œç¹¼çºŒè™•ç†
                
            # å˜—è©¦ç²å–åŸå§‹å¤§å°çš„åœ–ç‰‡URL
            original_src = src
            if "/thumb/" in src and "px-" in src:
                # å¾ç¸®ç•¥åœ–URLæ¨å°å‡ºåŸå§‹åœ–ç‰‡URL
                # ä¾‹å¦‚: //upload.wikimedia.org/wikipedia/commons/thumb/5/57/20230630_Yeh_Shu-hua.jpg/250px-20230630_Yeh_Shu-hua.jpg
                # è®Šæˆ: //upload.wikimedia.org/wikipedia/commons/5/57/20230630_Yeh_Shu-hua.jpg
                parts = src.split("/thumb/")
                if len(parts) == 2:
                    # ç¬¬äºŒéƒ¨åˆ†æ‡‰è©²æ˜¯ "5/57/20230630_Yeh_Shu-hua.jpg/250px-20230630_Yeh_Shu-hua.jpg"
                    # æˆ‘å€‘éœ€è¦å»æ‰æœ€å¾Œçš„å°ºå¯¸éƒ¨åˆ†ï¼Œåªä¿ç•™ "5/57/20230630_Yeh_Shu-hua.jpg"
                    thumb_part = parts[1]
                    # æ‰¾åˆ°æœ€å¾Œä¸€å€‹ "/" ä¸¦å»æ‰å¾Œé¢çš„å°ºå¯¸ç‰ˆæœ¬
                    last_slash = thumb_part.rfind("/")
                    if last_slash != -1:
                        file_path = thumb_part[:last_slash]  # "5/57/20230630_Yeh_Shu-hua.jpg"
                        original_src = parts[0] + "/" + file_path
            
            # ç²å–åœ–ç‰‡æè¿°
            alt_text = img.get("alt") or "" if hasattr(img, 'get') else ""
            title_text = img.get("title") or "" if hasattr(img, 'get') else ""
            
            # è½‰æ›ç‚ºå­—ç¬¦ä¸²
            alt_text = str(alt_text) if alt_text else ""
            title_text = str(title_text) if title_text else ""
            
            # æŸ¥æ‰¾åœ–ç‰‡èªªæ˜æ–‡å­—
            caption = ""
            
            # æª¢æŸ¥æ˜¯å¦åœ¨ .tmulti å®¹å™¨ä¸­
            tmulti_container = img.find_parent("div", class_="tmulti")
            if tmulti_container:
                # å…ˆæŸ¥æ‰¾è©²åœ–ç‰‡å°æ‡‰çš„ .thumbcaption
                tsingle = img.find_parent("div", class_="tsingle")
                if tsingle:
                    thumbcaption = tsingle.find("div", class_="thumbcaption")
                    if thumbcaption:
                        caption = thumbcaption.get_text(strip=True)
                
                # å¦‚æœæ²’æ‰¾åˆ°ï¼Œæˆ–è€…åªæ˜¯é€šç”¨æ–‡å­—ï¼ŒæŸ¥æ‰¾æ•´å€‹ tmulti å®¹å™¨çš„ç¸½é«”èªªæ˜
                if not caption or caption == "åœ–ç‰‡":
                    # æŸ¥æ‰¾ tmulti å®¹å™¨åº•éƒ¨çš„èªªæ˜æ–‡å­—
                    # æ–¹æ³•1: æŸ¥æ‰¾ thumbinner å¾Œé¢çš„ div
                    thumbinner = tmulti_container.find("div", class_="thumbinner")
                    if thumbinner:
                        # æŸ¥æ‰¾åŒç´šçš„èªªæ˜å…ƒç´ 
                        for sibling in thumbinner.find_next_siblings():
                            if hasattr(sibling, 'get_text'):
                                sibling_text = sibling.get_text(strip=True)
                                if sibling_text:
                                    print(f"æ‰¾åˆ° tmulti sibling æ–‡å­—: {sibling_text}")
                                    caption = extract_specific_caption_from_tmulti(img, sibling_text, tmulti_container)
                                    if caption:
                                        print(f"æå–åˆ°çš„ caption: {caption}")
                                        break
                    
                    # æ–¹æ³•2: å¦‚æœé‚„æ²’æ‰¾åˆ°ï¼Œåœ¨æ•´å€‹ tmulti å®¹å™¨ä¸­æŸ¥æ‰¾æ‰€æœ‰æ–‡å­—å…§å®¹
                    if not caption or caption == "åœ–ç‰‡":
                        tmulti_text = tmulti_container.get_text(strip=True)
                        print(f"tmulti å®Œæ•´æ–‡å­—: {tmulti_text[:200]}...")
                        if tmulti_text:
                            caption = extract_specific_caption_from_tmulti(img, tmulti_text, tmulti_container)
                            if caption:
                                print(f"å¾å®Œæ•´æ–‡å­—æå–åˆ°çš„ caption: {caption}")
                                    
                    # æ–¹æ³•3: åœ¨åŒä¸€å€‹ infobox-full-data ä¸­æŸ¥æ‰¾èªªæ˜
                    if not caption or caption == "åœ–ç‰‡":
                        infobox_data = tmulti_container.find_parent("td", class_="infobox-full-data")
                        if infobox_data:
                            # æŸ¥æ‰¾èªªæ˜æ–‡å­—ï¼ˆé€šå¸¸åœ¨åœ–ç‰‡å®¹å™¨å¾Œï¼‰
                            for child in infobox_data.children:
                                if hasattr(child, 'get_text') and child != tmulti_container:
                                    child_text = child.get_text(strip=True)
                                    if child_text and len(child_text) > 10:  # éæ¿¾çŸ­æ–‡å­—
                                        print(f"æ‰¾åˆ° infobox-data æ–‡å­—: {child_text}")
                                        caption = extract_specific_caption_from_tmulti(img, child_text, tmulti_container)
                                        if caption:
                                            print(f"å¾ infobox-data æå–åˆ°çš„ caption: {caption}")
                                            break
            
            # å¦‚æœä¸åœ¨ tmulti å®¹å™¨ä¸­ï¼Œæˆ–è€…æ²’æ‰¾åˆ° captionï¼Œä½¿ç”¨åŸæœ‰é‚è¼¯
            if not caption:
                # åœ¨æ•´å€‹ infobox ä¸­æŸ¥æ‰¾èˆ‡æ­¤åœ–ç‰‡ç›¸é—œçš„èªªæ˜æ–‡å­—
                infobox = cell.find_parent("table", class_="infobox") if cell else None
                if infobox:
                    # æŸ¥æ‰¾ infobox-caption å…ƒç´ 
                    caption_elements = infobox.find_all("div", class_="infobox-caption")
                    for cap_elem in caption_elements:
                        # æª¢æŸ¥èªªæ˜æ–‡å­—æ˜¯å¦èˆ‡æ­¤åœ–ç‰‡åœ¨åŒä¸€å€‹å–®å…ƒæ ¼æˆ–ç›¸è¿‘ä½ç½®
                        caption_text = cap_elem.get_text(strip=True)
                        if caption_text:
                            caption = caption_text
                            break
                
                # å¦‚æœæ²’æ‰¾åˆ°ï¼Œå˜—è©¦å¾çˆ¶ç´šå…ƒç´ ç²å–
                if not caption:
                    parent = img.find_parent()
                    if parent and hasattr(parent, 'name') and parent.name in ["td", "th"]:
                        cell_text = parent.get_text(strip=True)
                        # ç§»é™¤åœ–ç‰‡çš„altæ–‡å­—ä¾†ç²å–èªªæ˜
                        if alt_text and alt_text in cell_text:
                            caption = cell_text.replace(alt_text, "").strip()
                        else:
                            caption = cell_text
            
            # é¸æ“‡æœ€åˆé©çš„æè¿°æ–‡å­—
            description = caption or title_text or alt_text or "åœ–ç‰‡"
            
            # æ¸…ç†æè¿°æ–‡å­—
            description = re.sub(r"\s+", " ", description.strip())
            if len(description) > 100:
                description = description[:100] + "..."
            
            # ä¸‹è¼‰åœ–ç‰‡ - ä½¿ç”¨åŸå§‹å¤§å°çš„URL
            filename = download_image(original_src, images_dir, session)
            
            if filename and description:
                # æ·»åŠ åˆ°JSONLè³‡è¨Š
                image_info_json.append({
                    "title": title,
                    "image_url": original_src,
                    "source_url": source_url,
                    "image_filename": filename,
                    "caption": description
                })
    
    return image_info_text, image_info_json


def html_to_text(html: str, title: str = "", source_url: str = "", images_dir: Optional[Path] = None, 
                 session: Optional[requests.Session] = None, exclude_sections: list[str] | None = None) -> Tuple[str, list[dict]]:
    """
    æ®µè½åŒ–è¼¸å‡ºï¼ˆREST / Action çš†å¯ï¼‰ï¼š
    - å™ªéŸ³å…ˆç§»é™¤
    - æŠ“å–å°èˆªæ¡†ä¸­çš„åœ–ç‰‡è³‡è¨Š
    - é€æƒ h2/h3/p/ul/olï¼›é‡åˆ°è¢«æ’é™¤ç« ç¯€çš„ h2 å¾Œç›´åˆ°ä¸‹ä¸€å€‹ h2 å…¨è·³é
    - **H2** ä¸Šä¸‹å„ç•™ 1 ç©ºç™½è¡Œï¼›å…¶é¤˜è¡Œå–®æ›è¡Œ
    - è‹¥åµæ¸¬åˆ°ã€Œ<æ¨™ç±¤>ï¼šã€å¾Œå…§å®¹ç¼ºå¤±ï¼Œå¾è©²æ®µ DOM è£¡è£œæŠ“å€¼
    - æœ€å¾Œå‘¼å« zh_tidy åšæ¨™é»èˆ‡ç©ºç™½æ”¶æ–‚
    è¿”å›: (è™•ç†å¾Œçš„æ–‡æœ¬, åœ–ç‰‡è³‡è¨ŠJSONLåˆ—è¡¨)
    """
    soup = BeautifulSoup(html, "html.parser")

    # åœ¨ç§»é™¤å°èˆªæ¡†ä¹‹å‰ï¼Œå…ˆæå–åœ–ç‰‡è³‡è¨Š
    nav_images_text, nav_images_json = [], []
    if images_dir and session:
        nav_images_text, nav_images_json = extract_images_from_infoboxes(
            soup, title, source_url, images_dir, session)

    # å™ªéŸ³æ¸…é™¤
    for tag in soup.select("style, script, noscript"):
        tag.decompose()
    for sel in [
        "sup.reference", "span.mw-editsection", "table.infobox", "table.navbox",
        "div.reflist", "ol.references", "div.metadata",
        # æ¸…é™¤ç¶­åŸºç™¾ç§‘è­¦å‘Šæ¡†å’Œç·¨è¼¯æç¤º
        "div.ambox", "div.mbox-small", "div.messagebox", "table.ambox",
        "div.hatnote", "div.dablink", "div.rellink",
        # æ¸…é™¤ infobox ç›¸é—œ
        "table.infobox", "div.infobox", ".infobox",
        # æ¸…é™¤æ¨¡æ¿å’Œç·¨è¼¯ç›¸é—œ  
        "div.navbox", "table.navbox",
        # æ¸…é™¤å´é‚Šå°èˆªæ¡†
        "table.sidebar", "div.sidebar", ".sidebar"
    ]:
        for tag in soup.select(sel):
            tag.decompose()
    
    # ç‰¹æ®Šè™•ç†å¯æ‘ºç–Šå…§å®¹ - åªç§»é™¤å°èˆªç”¨çš„æ‘ºç–Šå€å¡Šï¼Œä¿ç•™å…§å®¹ç”¨çš„æ‘ºç–Šå€å¡Š
    for collapsible in soup.select("div.mw-collapsible"):
        # å¦‚æœæ˜¯ navbox ç›¸é—œçš„æ‘ºç–Šå…§å®¹ï¼Œå‰‡ç§»é™¤
        if collapsible.find_parent("div", class_="navbox") or "navbox" in " ".join(collapsible.get("class", [])):
            collapsible.decompose()
        # å¦‚æœæ˜¯åœ¨è¡¨æ ¼å…§çš„æ‘ºç–Šå…§å®¹ï¼ˆé€šå¸¸æ˜¯å°ˆè¼¯æ›²ç›®ï¼‰ï¼Œå‰‡å±•é–‹è€Œä¸ç§»é™¤
        elif collapsible.find_parent("table"):
            # ç§»é™¤æ‘ºç–Šçš„æ¨£å¼ï¼Œè®“å…§å®¹å®Œå…¨å±•é–‹
            collapsible["class"] = [c for c in collapsible.get("class", []) if c not in ["mw-collapsed", "mw-collapsible"]]

    # é¸æ“‡æ­£ç¢ºçš„mw-parser-outputï¼ˆåœ¨mw-content-textè£¡é¢çš„ä¸»è¦å…§å®¹ï¼Œè€Œä¸æ˜¯åæ¨™æŒ‡ç¤ºå™¨è£¡çš„ï¼‰
    root = soup.select_one("#mw-content-text .mw-parser-output") or soup.select_one("div.mw-parser-output") or soup.body or soup
    elements = root.find_all(["h2", "h3", "p", "ul", "ol", "dl", "table"], recursive=True)

    exclude = set(exclude_sections or [])
    lines: list[str] = []
    skipping = False

    def norm_title(s: str) -> str:
        return re.sub(r"\[.*?\]", "", s).strip()

    def squeeze(s: str) -> str:
        return re.sub(r"[ \t\u00A0]+", " ", s.strip())

    # è¿½è¹¤å·²è™•ç†çš„æ¨™é¡Œï¼Œé¿å…é‡è¤‡
    processed_titles = set()
    
    def add_title_if_new(title_text, prefix=""):
        """åªåœ¨æ¨™é¡Œæ²’æœ‰é‡è¤‡æ™‚æ‰æ·»åŠ """
        if not title_text:
            return False
        
        # æ¨™æº–åŒ–æ¨™é¡Œç”¨æ–¼æ¯”è¼ƒï¼ˆç§»é™¤å‰ç¶´å’Œç©ºç™½ï¼‰
        normalized = title_text.replace("###", "").replace("##", "").strip()
        if normalized in processed_titles:
            return False
        
        processed_titles.add(normalized)
        if lines and lines[-1] != "":
            lines.append("")
        lines.append(f"{prefix}{title_text}")
        return True

    for el in elements:
        if el.name == "h2":
            title = norm_title(smart_text(el))
            if any(key in title for key in exclude):
                skipping = True
                continue
            skipping = False
            if add_title_if_new(title, "## "):
                lines.append("")          # H2 å¾Œç©ºè¡Œ
            continue

        if el.name == "h3":
            if skipping:
                continue
            # è·³éå·²è¢« multicol è¡¨æ ¼è™•ç†éçš„ h3
            if el.find_parent("table", class_="multicol"):
                continue
            title = norm_title(smart_text(el))
            add_title_if_new(title, "### ")
            continue

        if skipping:
            continue

        if el.name == "p":
            txt = squeeze(smart_text(el))
            if txt:
                # æª¢æŸ¥æ˜¯å¦æ˜¯ç°¡çŸ­çš„æ¨™é¡Œå‹æ®µè½ï¼ˆå¦‚ã€Œå€‹äººæ¦®è­½ã€ï¼‰
                is_likely_title = (
                    len(txt) < 50 and 
                    not txt.endswith(('ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?')) and
                    not any(char in txt for char in 'ï¼Œ,ã€ï¼›;:ï¼š')
                )
                
                if is_likely_title:
                    # ç•¶ä½œæ¨™é¡Œè™•ç†
                    lines.append(txt)
                else:
                    # é€šç”¨æ¨™ç±¤è£œå€¼ï¼ˆå°±åœ° DOM æƒæï¼‰
                    txt = fill_labels_if_missing(el, txt)
                    lines.append(txt)
        elif el.name in ("ul", "ol"):
            # è·³éå·²è¢«è¡¨æ ¼è™•ç†éçš„ ul/ol 
            if el.find_parent("table"):
                continue
            items = []
            for li in el.find_all("li", recursive=False):
                li_txt = squeeze(smart_text(li))
                if li_txt:
                    items.append(f"â€¢ {li_txt}")
            if items:
                lines.extend(items)
        elif el.name == "dl":
            # è™•ç†å®šç¾©åˆ—è¡¨ (definition list)
            # è·³éå·²è¢«è¡¨æ ¼è™•ç†éçš„ dl
            if el.find_parent("table"):
                continue
            for child in el.find_all(["dt", "dd"], recursive=False):
                if child.name == "dt":
                    # dt ä½œç‚ºå°æ¨™é¡Œ
                    dt_txt = squeeze(smart_text(child))
                    if dt_txt:
                        lines.append(f"### {dt_txt}")
                elif child.name == "dd":
                    # å¦‚æœ dd åŒ…å«è¡¨æ ¼ï¼Œè·³éå®ƒï¼ˆè¡¨æ ¼æœƒå–®ç¨è™•ç†ï¼‰
                    if child.find("table"):
                        continue
                    # dd ä½œç‚ºå…§å®¹æ®µè½
                    dd_txt = squeeze(smart_text(child))
                    if dd_txt:
                        lines.append(dd_txt)
        elif el.name == "table":
            # åªè™•ç†å…§å®¹è¡¨æ ¼ï¼Œè·³é infoboxã€navboxã€ambox ç­‰
            if el.find_parent("table") is not None:
                continue
            classes = " ".join(el.get("class", [])).lower()
            if any(cls in classes for cls in ["infobox", "navbox", "ambox", "mbox", "messagebox"]):
                continue
            
            # ç‰¹æ®Šè™•ç† multicol è¡¨æ ¼ï¼ˆå¦‚éŸ³æ¨‚ä½œå“æ¬„ä½ï¼‰
            if "multicol" in classes:
                multicol_lines = process_multicol_table(el, squeeze)
                if multicol_lines:
                    lines.extend(multicol_lines)
            else:
                table_lines = table_to_lines(el, squeeze)
                if table_lines:
                    lines.extend(table_lines)

    text = "\n".join(lines)
    
    # å¾Œè™•ç†ï¼šç§»é™¤é€£çºŒé‡è¤‡çš„æ¨™é¡Œ
    text = remove_duplicate_headings(text)
    
    # å¾Œè™•ç†ï¼šåˆ†é›¢å¯èƒ½é€£åœ¨ä¸€èµ·çš„æ¨™é¡Œ
    text = separate_concatenated_titles(text)
    
    # æ¸…é™¤å­˜æª”å‚™ä»½æ–‡å­—
    text = remove_archive_links(text)
    
    text = zh_tidy(text)
    return text, nav_images_json


def remove_duplicate_headings(text: str) -> str:
    """
    ç§»é™¤é€£çºŒé‡è¤‡çš„æ¨™é¡Œï¼Œä¾‹å¦‚ï¼š
    ### å‰ä»–
    ### å‰ä»–
    ### å‰ä»–
    -> åªä¿ç•™ä¸€å€‹ ### å‰ä»–
    """
    lines = text.split('\n')
    result_lines = []
    last_heading = None
    
    for line in lines:
        stripped = line.strip()
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ¨™é¡Œè¡Œï¼ˆ## æˆ– ### é–‹é ­ï¼‰
        if stripped.startswith('###') or stripped.startswith('##'):
            # æ¨™æº–åŒ–æ¨™é¡Œé€²è¡Œæ¯”è¼ƒï¼ˆç§»é™¤å‰ç¶´å’Œå¤šé¤˜ç©ºç™½ï¼‰
            normalized = stripped.replace('###', '').replace('##', '').strip()
            
            # å¦‚æœèˆ‡ä¸Šä¸€å€‹æ¨™é¡Œä¸åŒï¼Œæˆ–è€…ä¸æ˜¯æ¨™é¡Œï¼Œå‰‡æ·»åŠ 
            if normalized != last_heading:
                result_lines.append(line)
                last_heading = normalized
            # å¦‚æœæ˜¯é‡è¤‡æ¨™é¡Œï¼Œè·³é
        else:
            # éæ¨™é¡Œè¡Œï¼Œç›´æ¥æ·»åŠ ï¼Œä¸¦é‡ç½®æ¨™é¡Œè¿½è¹¤
            result_lines.append(line)
            if stripped:  # éç©ºè¡Œæ‰é‡ç½®
                last_heading = None
    
    return '\n'.join(result_lines)


def separate_concatenated_titles(text: str) -> str:
    """
    åˆ†é›¢å¯èƒ½è¢«é€£åœ¨ä¸€èµ·çš„æ¨™é¡Œï¼Œä¾‹å¦‚ã€Œå½±éŸ³ä½œå“å…¶ä»–éŸ³æ¨‚éŒ„å½±å¸¶ã€-> ã€Œå½±éŸ³ä½œå“\nå…¶ä»–éŸ³æ¨‚éŒ„å½±å¸¶ã€
    """
    # å¸¸è¦‹çš„éœ€è¦åˆ†é›¢çš„æ¨™é¡Œæ¨¡å¼
    title_pairs = [
        ("å½±éŸ³ä½œå“", "å…¶ä»–éŸ³æ¨‚éŒ„å½±å¸¶"),
        ("å€‹äººç”Ÿæ´»", "æ„Ÿæƒ…ç‹€æ³"),
        ("æ¼”è—ç¶“æ­·", "éŸ³æ¨‚ä½œå“"),
        ("ä½œå“åˆ—è¡¨", "éŸ³æ¨‚ä½œå“"),
        ("ç²çè¨˜éŒ„", "å€‹äººæ¦®è­½"),
    ]
    
    for title1, title2 in title_pairs:
        # æŸ¥æ‰¾é€£åœ¨ä¸€èµ·çš„æ¨™é¡Œ
        combined = title1 + title2
        if combined in text:
            # æ›¿æ›ç‚ºåˆ†è¡Œçš„ç‰ˆæœ¬
            text = text.replace(combined, f"{title1}\n\n{title2}")
    
    return text


def remove_archive_links(text: str) -> str:
    """
    ç§»é™¤ç¶­åŸºç™¾ç§‘çš„å­˜æª”å‚™ä»½é€£çµæ–‡å­—
    ä¾‹å¦‚ï¼šï¼ˆé é¢å­˜æª”å‚™ä»½ï¼Œå­˜æ–¼ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨ï¼‰
    """
    # ç§»é™¤å­˜æª”å‚™ä»½æ–‡å­—çš„å¤šç¨®è®Šé«”
    patterns = [
        r"ï¼ˆé é¢å­˜æª”å‚™ä»½ï¼Œå­˜æ–¼ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨ï¼‰",
        r"\(é é¢å­˜æª”å‚™ä»½ï¼Œå­˜æ–¼ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨\)",
        r"ï¼ˆ\s*é é¢å­˜æª”å‚™ä»½\s*ï¼Œ\s*å­˜æ–¼\s*ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨\s*ï¼‰",
        r"\(\s*é é¢å­˜æª”å‚™ä»½\s*ï¼Œ\s*å­˜æ–¼\s*ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨\s*\)",
    ]
    
    for pattern in patterns:
        text = re.sub(pattern, "", text, flags=re.IGNORECASE)
    
    # æ¸…ç†å¤šé¤˜çš„ç©ºè¡Œå’Œç©ºæ ¼
    text = re.sub(r"\n\s*\n\s*\n", "\n\n", text)  # æ¸›å°‘å¤šé¤˜ç©ºè¡Œ
    text = re.sub(r" +", " ", text)  # å£“ç¸®å¤šé¤˜ç©ºæ ¼
    
    return text


# -------------------------------
# ä¸»æµç¨‹ï¼šå–®ç¯‡è™•ç†
# -------------------------------
def process_one(raw: str, kind: str, out_dir: Path, session: requests.Session,
                exclude_sections: list[str], jsonl_file, source_file: str = "", force: bool = False) -> Tuple[str, bool, str]:
    title = url_to_title(raw) if kind == "url" else raw
    txt_dir = out_dir / "txt"
    images_dir = out_dir / "images"
    images_jsonl = out_dir / "images" / "images_info.jsonl"

    requested_filename = safe_filename(title)
    requested_path = txt_dir / f"{requested_filename}.txt"
    # å¦‚æœ force=Trueï¼Œç„¡è«–æ–‡ä»¶æ˜¯å¦å­˜åœ¨éƒ½è¦è™•ç†ï¼›å¦‚æœ force=False ä¸”æ–‡ä»¶å­˜åœ¨ï¼Œå‰‡è·³é
    if not force and requested_path.exists():
        return title, True, "exists"

    # å…ˆ Actionï¼ˆvariant=zh-twï¼‰ï¼Œå¤±æ•—å† RESTï¼ˆå¸¶ Accept-Language: zh-twï¼‰
    html = None
    actual_title = title
    source_url = f"https://zh.wikipedia.org/wiki/{quote(title, safe='')}"
    
    try:
        html, actual_title = fetch_html_action(title, session)
    except Exception:
        html, actual_title = fetch_html_rest(title, session)

    # æª¢æŸ¥HTMLå±¤é¢çš„é‡å®šå‘ï¼ˆç‰¹åˆ¥æ˜¯REST APIè¿”å›çš„é‡å®šå‘é é¢ï¼‰
    html_redirect_target = detect_redirect_from_html(html)
    if html_redirect_target and html_redirect_target != title:
        print(f"ğŸ”„ æª¢æ¸¬åˆ°HTMLé‡å®šå‘ï¼š{title} -> {html_redirect_target}")
        source_url = f"https://zh.wikipedia.org/wiki/{quote(html_redirect_target, safe='')}"
        
        try:
            html, actual_title = fetch_html_action(html_redirect_target, session)
        except Exception:
            try:
                html, actual_title = fetch_html_rest(html_redirect_target, session)
            except Exception as e:
                raise RuntimeError(f"é‡å®šå‘ç›®æ¨™é é¢æŠ“å–å¤±æ•—: {html_redirect_target}, éŒ¯èª¤: {e}")

    text, image_info = html_to_text(html, title=actual_title, source_url=source_url, 
                                   images_dir=images_dir, session=session, 
                                   exclude_sections=exclude_sections)

    redirect_target = detect_redirect(text)
    if redirect_target and redirect_target != title:
        print(f"ğŸ”„ æª¢æ¸¬åˆ°é‡å®šå‘ï¼š{title} -> {redirect_target}")
        source_url = f"https://zh.wikipedia.org/wiki/{quote(redirect_target, safe='')}"
        
        try:
            html, actual_title = fetch_html_action(redirect_target, session)
        except Exception:
            try:
                html, actual_title = fetch_html_rest(redirect_target, session)
            except Exception as e:
                raise RuntimeError(f"é‡å®šå‘ç›®æ¨™é é¢æŠ“å–å¤±æ•—: {redirect_target}, éŒ¯èª¤: {e}")

        text, image_info = html_to_text(html, title=actual_title, source_url=source_url,
                                       images_dir=images_dir, session=session,
                                       exclude_sections=exclude_sections)

    # å…¨æ–‡å…œåº•ï¼šè‹¥ä»æœ‰ã€Œ<èªè¨€æ¨™ç±¤>ï¼š<ç¼ºå€¼>ã€ï¼Œç”¨ langlinks è£œ
    text = ensure_labels_after_marker(text, title=actual_title, session=session)

    # æª¢æŸ¥æ–‡æœ¬é•·åº¦ï¼Œå¦‚æœç‚º 0 å‰‡æ‹‹å‡ºéŒ¯èª¤
    if len(text.strip()) == 0:
        raise RuntimeError(f"æŠ“å–çš„æ–‡æœ¬é•·åº¦ç‚º 0ï¼š{actual_title}")

    final_filename = safe_filename(actual_title)
    out_txt = txt_dir / f"{final_filename}.txt"
    status = "redirect_ok" if redirect_target else "ok"

    # å¦‚æœ force=Trueï¼Œç„¡è«–æ–‡ä»¶æ˜¯å¦å­˜åœ¨éƒ½è¦è¦†è“‹ï¼›å¦‚æœ force=False ä¸”æ–‡ä»¶å­˜åœ¨ï¼Œå‰‡è·³é
    if not force and out_txt.exists():
        if redirect_target:
            return actual_title, True, "redirect_exists"
        return actual_title, True, "exists"

    out_txt.parent.mkdir(parents=True, exist_ok=True)
    out_txt.write_text(text, encoding="utf-8")

    # ä¿å­˜åœ–ç‰‡è³‡è¨Šåˆ°JSONLæ–‡ä»¶
    if image_info:
        images_jsonl.parent.mkdir(parents=True, exist_ok=True)
        with images_jsonl.open("a", encoding="utf-8") as img_file:
            for img_data in image_info:
                img_file.write(json.dumps(img_data, ensure_ascii=False) + "\n")

    rec = {
        "title": actual_title,
        "original_query": title,
        "source_url": f"https://zh.wikipedia.org/wiki/{quote(actual_title, safe='')}",
        "variant": "zh-tw",
        "source_title_lang": "zh-tw",
        "text_length": len(text),
        "text": text,
        "out_file": str(Path("txt") / f"{final_filename}.txt"),
        "source_file": source_file,  # æ–°å¢ä¾†æºæª”æ¡ˆè³‡è¨Š
    }
    if redirect_target:
        rec["redirected_from"] = title
        rec["redirected_to"] = actual_title
    
    # æ·»åŠ åœ–ç‰‡è³‡è¨Šåˆ°è¨˜éŒ„ä¸­
    if image_info:
        rec["images_count"] = len(image_info)
        rec["images"] = [{"filename": img["image_filename"], "caption": img["caption"]} 
                        for img in image_info]

    if jsonl_file is not None:
        jsonl_file.write(json.dumps(rec, ensure_ascii=False) + "\n")
        jsonl_file.flush()

    return actual_title, True, status


# -------------------------------
# CLI å…¥å£
# -------------------------------
def main():
    ap = argparse.ArgumentParser(description="æ‰¹æ¬¡æŠ“å–ä¸­æ–‡ç¶­åŸºï¼ˆè‡ºç£æ­£é«” zh-TWï¼‰")
    ap.add_argument("--targets", required=True, help="ç›®æ¨™æ¸…å–®è·¯å¾‘ï¼š.txt æˆ– .jsonl æª”æ¡ˆï¼Œæˆ–åŒ…å«å¤šå€‹ .txt/.jsonl æª”æ¡ˆçš„è³‡æ–™å¤¾è·¯å¾‘")
    ap.add_argument("--out-dir", default="out", help="è¼¸å‡ºè³‡æ–™å¤¾")
    ap.add_argument("--sleep", type=float, default=0.5, help="æ¯ç¯‡ä¹‹é–“çš„å»¶é²ç§’æ•¸ï¼ˆç¦®è²ŒæŠ“å–ï¼‰")
    ap.add_argument("--ua", default=DEFAULT_UA, help="è‡ªè¨‚ User-Agentï¼ˆè«‹å¡«å¯è¯çµ¡è³‡è¨Šï¼‰")
    ap.add_argument("--force", action="store_true", help="å¼·åˆ¶é‡æ–°ä¸‹è¼‰å·²å­˜åœ¨çš„æª”æ¡ˆ")
    ap.add_argument(
        "--exclude-sections",
        # zh è®Šé«”æœƒæŠŠã€Œç›¸å…³æ¡ç›®/æ‰©å±•é˜…è¯»ã€è‡ªå‹•è½‰ç‚ºã€Œç›¸é—œæ¢ç›®/æ“´å±•é–±è®€ã€
        default="å¼•ç”¨è³‡æ–™,åƒè€ƒæ›¸ç›®,ç›¸é—œå­¸è¡“ç ”ç©¶æ›¸ç›®,åƒè€ƒ,åƒè€ƒä¾†æº,åƒè€ƒè³‡æ–™,å¤–éƒ¨é€£çµ,ç›¸é—œæ¢ç›®,æ“´å±•é–±è®€,å»¶ä¼¸é–±è®€,åƒè¦‹,åƒè€ƒæ–‡ç»,è…³è¨»,è¨»é‡‹,è¨»è§£,æ³¨è§£,å‚™è¨»,é—œè¯é …ç›®,è³‡æ–™ä¾†æº,æ³¨é‡‹,è¨»è…³,æ³¨è…³,é—œé€£é …ç›®,å‚™æ³¨,å‚™è¨»",
        help="è¦æ’é™¤çš„ç« ç¯€æ¨™é¡Œï¼ˆä»¥é€—è™Ÿåˆ†éš”ï¼‰",
    )
    args = ap.parse_args()

    targets_path = Path(args.targets)
    if not targets_path.exists():
        print(f"âŒ ç›®æ¨™è·¯å¾‘ä¸å­˜åœ¨: {targets_path}")
        return
        
    out_dir = Path(args.out_dir)
    exclude_sections = [s.strip() for s in args.exclude_sections.split(",") if s.strip()]

    S = requests.Session()
    S.headers.update({
        "User-Agent": args.ua,
        # è®“ REST/Parsoid ä¾è®Šé«”è¼¸å‡ºè‡ºç£æ­£é«”ï¼›èˆ‡ Action API çš„ variant ç›¸è¼”ç›¸æˆ
        "Accept-Language": "zh-tw",
    })

    ok, skip, fail = 0, 0, 0
    current_source_file = ""
    failures_log = out_dir / "_failures.jsonl"
    txt_dir = out_dir / "txt"
    json_dir = out_dir / "jsonl"
    images_dir = out_dir / "images"
    all_data_jsonl = json_dir / "all_data.jsonl"
    images_jsonl = images_dir / "images_info.jsonl"

    out_dir.mkdir(parents=True, exist_ok=True)
    json_dir.mkdir(parents=True, exist_ok=True)
    
    # å…ˆè¨ˆç®—ç¸½æ•¸é‡ï¼Œä»¥ä¾¿é¡¯ç¤ºé€²åº¦
    all_targets = list(iter_targets(targets_path))
    total_count = len(all_targets)
    processed_count = 0
    
    print(f"ğŸš€ é–‹å§‹è™•ç† {total_count} å€‹ç›®æ¨™")
    
    import io
    for raw, kind, source_file in all_targets:
        processed_count += 1
        
        # é¡¯ç¤ºç•¶å‰è™•ç†çš„æª”æ¡ˆï¼ˆå¦‚æœè®Šæ›´ï¼‰
        if source_file != current_source_file:
            current_source_file = source_file
            print(f"\nğŸ“‚ ç•¶å‰è™•ç†æª”æ¡ˆ: {Path(source_file).name}")
        
        try:
            # ä½¿ç”¨ StringIO æš«å­˜ jsonl è¨˜éŒ„
            rec_io = io.StringIO()
            title, success, msg = process_one(raw, kind, out_dir, S, exclude_sections, rec_io, source_file, args.force)
            
            # è§£ææ–°ç”¢ç”Ÿçš„è¨˜éŒ„
            rec_io.seek(0)
            new_rec = None
            for line in rec_io:
                if line.strip():
                    new_rec = json.loads(line)
                    break
            
            # åªæœ‰æˆåŠŸè™•ç†ä¸”æœ‰æ–°è¨˜éŒ„æ™‚æ‰å¯«å…¥ jsonl
            if new_rec and success and msg not in ["exists", "redirect_exists"]:
                if args.force:
                    # force æ¨¡å¼ï¼šè®€å–ç¾æœ‰ jsonlï¼Œç§»é™¤åŒ title çš„èˆŠè¨˜éŒ„ï¼Œå†å¯«å›å…¨éƒ¨
                    all_jsonl = []
                    if all_data_jsonl.exists():
                        with all_data_jsonl.open("r", encoding="utf-8") as f:
                            for line in f:
                                if line.strip():
                                    try:
                                        obj = json.loads(line)
                                        if obj.get("title") != title:
                                            all_jsonl.append(obj)
                                    except Exception:
                                        pass
                    all_jsonl.append(new_rec)
                    with all_data_jsonl.open("w", encoding="utf-8") as f:
                        for obj in all_jsonl:
                            f.write(json.dumps(obj, ensure_ascii=False) + "\n")
                else:
                    # é force æ¨¡å¼ï¼šç›´æ¥è¿½åŠ 
                    with all_data_jsonl.open("a", encoding="utf-8") as f:
                        f.write(json.dumps(new_rec, ensure_ascii=False) + "\n")
            
            if msg == "exists":
                skip += 1
                print(f"â­ï¸  [{processed_count}/{total_count}] ç•¥éï¼ˆå·²å­˜åœ¨ï¼‰ï¼š{title}")
            elif msg == "redirect_exists":
                skip += 1
                print(f"â­ï¸  [{processed_count}/{total_count}] ç•¥éï¼ˆé‡å®šå‘ç›®æ¨™å·²å­˜åœ¨ï¼‰ï¼š{title}")
            elif msg == "redirect_ok":
                ok += 1
                print(f"âœ… [{processed_count}/{total_count}] å®Œæˆï¼ˆé‡å®šå‘ï¼‰ï¼š{raw} -> {title}")
            else:
                ok += 1
                print(f"âœ… [{processed_count}/{total_count}] å®Œæˆï¼š{title}")
        except Exception as e:
            fail += 1
            print(f"âŒ  [{processed_count}/{total_count}] å¤±æ•—ï¼š{raw}  -> {e}")
            failures_log.parent.mkdir(parents=True, exist_ok=True)
            with failures_log.open("a", encoding="utf-8") as f:
                f.write(json.dumps({"raw": raw, "kind": kind, "source_file": source_file, "error": str(e)}, ensure_ascii=False) + "\n")
        time.sleep(args.sleep)

    print("\n=== çµ±è¨ˆ ===")
    print(f"æˆåŠŸï¼š{ok}  å·²å­˜åœ¨ï¼š{skip}  å¤±æ•—ï¼š{fail}")
    print(f"æ–‡å­—è¼¸å‡ºï¼š{txt_dir.resolve()}")
    print(f"JSONL è¼¸å‡ºï¼š{all_data_jsonl.resolve()}")
    if images_jsonl.exists():
        print(f"åœ–ç‰‡è³‡è¨Šï¼š{images_jsonl.resolve()}")
        print(f"åœ–ç‰‡ä¸‹è¼‰ï¼š{images_dir.resolve()}")


if __name__ == "__main__":
    main()
