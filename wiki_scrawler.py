#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ‰¹æ¬¡æŠ“å–ä¸­æ–‡ç¶­åŸºç™¾ç§‘æ¢ç›®ï¼ˆä»¥ã€Œè‡ºç£æ­£é«” zh-TWã€è®Šé«”è¼¸å‡ºï¼‰
- ç›®æ¨™æ¸…å–®å¯ç‚º .txtï¼ˆæ¯è¡Œä¸€å€‹ URL æˆ–æ¨™é¡Œï¼‰æˆ– .jsonlï¼ˆå« {title|url}ï¼‰
- ä¸ä½¿ç”¨ OpenCCï¼›å®Œå…¨ä¾ MediaWiki èªè¨€è®Šé«”ï¼ˆAccept-Language / variantï¼‰å–å¾—å…§å®¹
- å·²å­˜åœ¨è¼¸å‡ºè‡ªå‹•ç•¥éï¼Œå¯æ–·é»çºŒæŠ“
- æ”¯æ´æ’é™¤æŒ‡å®šç« ç¯€ï¼ˆé è¨­ï¼šåƒè€ƒè³‡æ–™,å¤–éƒ¨é€£çµ,ç›¸é—œæ¢ç›®,æ“´å±•é–±è®€,å»¶ä¼¸é–±è®€,åƒè¦‹ï¼‰
- æ®µè½åŒ–è¼¸å‡ºï¼šH2 ä¸Šä¸‹å„ç•™ 1 ç©ºè¡Œï¼›å…¶é¤˜å–®è¡Œç›¸æ¥ï¼›æ¸…å–®é …ç›®ç”¨ã€Œâ€¢ ã€
- è‡ªå‹•ä¿®è£œã€Œ<æ¨™ç±¤>ï¼š<å€¼>ã€ç¼ºå€¼ï¼ˆè‹±èª/å­¸å/è—å/æœ¬å/åŸå/åˆ¥åâ€¦ï¼‰ï¼Œæ®µå…§ DOM æ“·å– + langlinks(en/ja/.../la) å…œåº•
- è‡ªå‹•ç§»é™¤ CJK èˆ‡æ¨™é»å‘¨é‚Šå¤šé¤˜ç©ºç™½ï¼Œä¿ç•™è‹±æ–‡å–®å­—å…§ç©ºç™½
"""

import argparse
import json
import re
import time
from pathlib import Path
from typing import Iterable, Tuple, Optional
from urllib.parse import urlparse, unquote, quote

import requests
from bs4 import BeautifulSoup, Tag, NavigableString

API = "https://zh.wikipedia.org/w/api.php"
REST_HTML = "https://zh.wikipedia.org/api/rest_v1/page/html/{title}"

DEFAULT_UA = "YourBotName/1.0 (contact@example.com)"  # å»ºè­°æ›æˆä½ çš„è³‡è¨Š


# -------------------------------
# è®€å–ç›®æ¨™æ¸…å–®
# -------------------------------
def iter_targets(path: Path) -> Iterable[Tuple[str, str]]:
    """
    è®€å– .txt æˆ– .jsonl ç›®æ¨™æ¸…å–®ã€‚
    ç”¢å‡º (raw, kind)ï¼škind = "url" æˆ– "title"
    """
    if path.suffix.lower() == ".jsonl":
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                obj = json.loads(line)
                if "url" in obj and obj["url"]:
                    yield obj["url"], "url"
                elif "title" in obj and obj["title"]:
                    yield obj["title"], "title"
    else:
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                s = line.strip()
                if not s or s.startswith("#"):
                    continue
                if s.startswith("http://") or s.startswith("https://"):
                    yield s, "url"
                else:
                    yield s, "title"


# -------------------------------
# å·¥å…·å‡½æ•¸
# -------------------------------
def url_to_title(url: str) -> str:
    path = urlparse(url).path
    title = unquote(path.rsplit("/", 1)[-1])
    return title

def safe_filename(title: str) -> str:
    name = re.sub(r'[\\/*?:"<>|]', "_", title)
    return name[:200]


def clean_display_title(title_html: str) -> str:
    if not title_html:
        return ""
    return BeautifulSoup(title_html, "html.parser").get_text(" ", strip=True)


def extract_display_title_from_html(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    meta = soup.find("meta", attrs={"property": "mw:displaytitle"})
    if meta and meta.get("content"):
        return meta["content"].strip()
    heading = soup.select_one("#firstHeading")
    if heading:
        return heading.get_text(" ", strip=True)
    title_tag = soup.find("title")
    if title_tag:
        return title_tag.get_text(" ", strip=True)
    return ""

def http_get_with_backoff(session: requests.Session, url: str, *, params=None,
                          timeout: int = 30, retries: int = 3, backoff_base: float = 2.0):
    last_exc: Optional[Exception] = None
    for i in range(retries + 1):
        try:
            r = session.get(url, params=params, timeout=timeout)
            if r.status_code in (429, 503):
                raise RuntimeError(f"HTTP {r.status_code}")
            # Parsoid / API ä¹Ÿå¯èƒ½ä»¥å­—ä¸²æç¤º maxlagï¼Œä¿å®ˆè™•ç†
            if r.status_code == 200 and "maxlag" in r.text.lower() and "error" in r.text.lower():
                raise RuntimeError("Server under high replication lag (maxlag).")
            r.raise_for_status()
            return r
        except Exception as e:
            last_exc = e
            if i == retries:
                break
            time.sleep(backoff_base ** i)  # 2,4,8...
    raise RuntimeError(f"GET failed for {url}: {last_exc}") from last_exc


# -------------------------------
# æŠ“å–ï¼ˆREST å„ªå…ˆï¼ŒAction API å¾Œå‚™ï¼‰
# -------------------------------
def fetch_html_rest(title: str, session: requests.Session, timeout=30) -> tuple[str, str]:
    url = REST_HTML.format(title=quote(title, safe=""))
    r = http_get_with_backoff(session, url, timeout=timeout)
    html = r.text
    display_title = extract_display_title_from_html(html)
    return html, (display_title or title)

def detect_redirect(text: str) -> Optional[str]:
    """
    æª¢æ¸¬é‡å®šå‘é é¢ï¼Œè¿”å›é‡å®šå‘çš„ç›®æ¨™æ¨™é¡Œ
    """
    # æª¢æ¸¬é‡å®šå‘æ¨™è¨˜
    redirect_patterns = [
        r"é‡å®šå‘åˆ°ï¼š\s*â€¢\s*([^\nâ€¢]+)",
        r"é‡æ–°å°å‘è‡³ï¼š\s*â€¢\s*([^\nâ€¢]+)", 
        r"#REDIRECT\s*\[\[([^\]]+)\]\]",
        r"#é‡å®šå‘\s*\[\[([^\]]+)\]\]"
    ]
    
    for pattern in redirect_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            target = match.group(1).strip()
            # æ¸…ç†å¯èƒ½çš„é¡å¤–æ¨™è¨˜
            target = re.sub(r'\|.*$', '', target)  # ç§»é™¤ç®¡é“é€£çµå¾Œçš„éƒ¨åˆ†
            return target
    
    return None


def fetch_html_action(title: str, session: requests.Session, timeout=30) -> Tuple[str, str]:
    """
    æŠ“å–é é¢å…§å®¹ï¼ŒåŒæ™‚è¿”å›å¯¦éš›çš„æ¨™é¡Œï¼ˆè™•ç†é‡å®šå‘ï¼‰
    è¿”å›: (html_content, actual_title)
    """
    params = {
        "action": "parse",
        "page": title,
        "prop": "text|displaytitle",
        "format": "json",
        "variant": "zh-tw",  # æŒ‡å®šè‡ºç£æ­£é«”è®Šé«”
        "maxlag": 5,
    }
    r = http_get_with_backoff(session, API, params=params, timeout=timeout)
    data = r.json()
    if "parse" not in data or "text" not in data["parse"]:
        raise RuntimeError(f"Action parse missing content for: {title}")
    
    # ç²å–å¯¦éš›çš„é é¢æ¨™é¡Œï¼ˆè™•ç†é‡å®šå‘ï¼‰
    display_title = clean_display_title(data["parse"].get("displaytitle"))
    actual_title = display_title or data["parse"].get("title", title)
    html_content = data["parse"]["text"]["*"]
    
    return html_content, actual_title


# -------------------------------
# æ’ç‰ˆæ•´ç†ï¼ˆTidyï¼‰
# -------------------------------
def zh_tidy(text: str) -> str:
    """
    ä¸­è‹±æ··æ’ç©ºç™½èˆ‡æ¨™é»æ•´ç†ï¼ˆä¿ç•™è‹±æ–‡å–®è©å…§ç©ºç™½ï¼›åªä¿®å‰ª CJK èˆ‡æ¨™é»é™„è¿‘ï¼‰ï¼š
    - æ›¸åè™Ÿ/å…§æ›¸åè™Ÿ/å¼•è™Ÿ/æ‹¬è™Ÿã€Œå…§å´ã€å»ç©ºç™½ï¼šã€Š ä½œå“ ã€‹â†’ã€Šä½œå“ã€‹ã€ã€ˆ æ—¥ä¸è½ ã€‰â†’ã€ˆæ—¥ä¸è½ã€‰ã€ï¼ˆ è‹±èªï¼šJolin Tsai ï¼‰â†’ï¼ˆè‹±èªï¼šJolin Tsaiï¼‰
    - æ¨™é»ã€Œå‰ã€å»ç©ºç™½ï¼šâ€¦â€¦ Jolin Tsai ï¼Œâ†’ â€¦â€¦ Jolin Tsaiï¼Œ
    - æ—¥æœŸã€Œå¹´/æœˆ/æ—¥ã€å»ç©ºç™½ï¼š1980å¹´ 9æœˆ 15æ—¥ â†’ 1980å¹´9æœˆ15æ—¥
    - ç ´æŠ˜è™Ÿ/å…©å­—ç ´æŠ˜è™Ÿå‘¨é‚Šå»ç©ºç™½ï¼š â€” â†’ â€”ã€ â€”â€” â†’ â€”â€”
    - CJK èˆ‡ CJK ä¹‹é–“çš„å¤šé¤˜ç©ºç™½å»é™¤ï¼›ã€Œã€ã€å‘¨é‚Šå»ç©ºç™½
    - å£“æ‰ 3 é€£ä»¥ä¸Šç©ºç™½è¡Œ
    """
    # 0) æ¨™æº–åŒ–ä¸å¯è¦‹ç©ºç™½
    text = re.sub(r"[ \t\u00A0]+", " ", text)

    # 1) æ‹¬è™Ÿ/æ›¸åè™Ÿ/å¼•è™Ÿ å…§å´ç©ºç™½
    pairs = [
        ("ã€Š", "ã€‹"), ("ã€ˆ", "ã€‰"),
        ("ã€Œ", "ã€"), ("ã€", "ã€"),
        ("ï¼ˆ", "ï¼‰")
    ]
    for l, r in pairs:
        text = re.sub(fr"{re.escape(l)}\s*(.*?)\s*{re.escape(r)}", fr"{l}\1{r}", text, flags=re.S)

    # 2) æ¨™é»ã€Œå‰ã€ä¸ç•™ç©ºç™½ï¼ˆä¸­æ–‡æ¨™é»ï¼‰
    text = re.sub(r"\s+([ï¼Œã€‚ã€ï¼›ï¼šï¼ï¼Ÿã€‹ï¼‰ã€ã€])", r"\1", text)

    # 3) æ—¥æœŸã€Œå¹´/æœˆ/æ—¥ã€ä¹‹é–“ä¸ç•™ç©ºç™½
    text = re.sub(r"(\d{1,4})\s*å¹´\s*(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥", r"\1å¹´\2æœˆ\3æ—¥", text)
    text = re.sub(r"(\d{1,4})\s*å¹´\s*(\d{1,2})\s*æœˆ", r"\1å¹´\2æœˆ", text)

    # 4) ç ´æŠ˜è™Ÿå‘¨é‚Šä¸ç•™ç©ºç™½ï¼ˆâ€” èˆ‡ â€”â€”ï¼‰
    text = re.sub(r"\s*â€”â€”\s*", "â€”â€”", text)
    text = re.sub(r"\s*â€”\s*", "â€”", text)

    # 5) CJK èˆ‡ CJK ä¹‹é–“å¤šé¤˜ç©ºç™½å»é™¤ï¼›ã€Œã€ã€å‘¨é‚Šå»ç©ºç™½
    CJK = r"\u4E00-\u9FFF\u3400-\u4DBF"
    # æ™ºèƒ½è™•ç†ï¼šä¿ç•™æ¨™é¡Œé–“çš„æ›è¡Œï¼Œä½†å»é™¤æ®µè½å…§çš„å¤šé¤˜ç©ºç™½
    # å…ˆè™•ç†æ¨™é¡Œè¡Œï¼ˆçŸ­è¡Œä¸”ç¨ç«‹ï¼‰
    lines = text.split('\n')
    for i, line in enumerate(lines):
        stripped = line.strip()
        # å¦‚æœæ˜¯çŸ­è¡Œï¼ˆå¯èƒ½æ˜¯æ¨™é¡Œï¼‰ï¼Œä¿æŒå–®ç¨æˆè¡Œ
        if stripped and len(stripped) < 30 and not any(char in stripped for char in 'ã€‚ï¼Œï¼ï¼Ÿ'):
            continue
        # å°æ–¼é•·æ®µè½ï¼Œå»é™¤ä¸­æ–‡å­—ç¬¦é–“çš„ç©ºæ ¼
        if stripped:
            lines[i] = re.sub(fr"(?<=[{CJK}])[ \t\u00A0]+(?=[{CJK}])", "", stripped)
    
    text = '\n'.join(lines)
    text = re.sub(r"\s*ã€\s*", "ã€", text)

    # 6) é€£çºŒ 3 è¡Œä»¥ä¸Šç©ºç™½ â†’ 2 è¡Œï¼ˆæœ€å¤šåªå…è¨±å…©å€‹æ›è¡Œï¼Œç”¨æ–¼å¤§æ¨™åˆ†éš”ï¼‰
    text = re.sub(r"\n{3,}", "\n\n", text)

    return text


# -------------------------------
# æ–‡å­—æŠ½å–ï¼ˆREST èˆ‡ Action çš†æ”¯æ´ï¼‰
# -------------------------------
def smart_text(node: Tag) -> str:
    """
    é€æ–‡å­—ç¯€é»ä¸²æ¥ï¼š
    - åªæœ‰ã€Œä¸Šä¸€å€‹å­—å…ƒã€èˆ‡ã€Œç•¶å‰ç‰‡æ®µç¬¬ä¸€å€‹å­—å…ƒã€éƒ½ç‚º ASCII å­—æ¯/æ•¸å­—æ™‚ï¼Œæ‰è£œ 1 å€‹ç©ºç™½
    - å…¶ä»–æƒ…æ³ç›´æ¥ç›¸é€£ï¼Œé¿å…åœ¨ CJK é‚Šç•Œè£½é€ å¤šé¤˜ç©ºç™½
    """
    parts = []
    last_ascii = False
    for d in node.descendants:
        if isinstance(d, NavigableString):
            s = str(d)
            if not s or not s.strip():
                continue
            s = s.strip()
            cur_ascii = bool(s and s[0].isascii())
            if parts and last_ascii and cur_ascii:
                parts.append(" ")
            parts.append(s)
            last_ascii = bool(s and s[-1].isascii())
        elif isinstance(d, Tag) and d.name == "br":
            parts.append("\n")
            last_ascii = False
    return "".join(parts)


def _parse_int(value, default: int = 1) -> int:
    try:
        return max(int(value), 1)
    except (TypeError, ValueError):
        return default


def table_to_lines(table: Tag, squeeze) -> list[str]:
    """Flatten a wiki table into pipe-separated rows with colspan/rowspan expansion."""
    lines: list[str] = []

    caption = table.find("caption")
    if caption:
        caption_text = squeeze(smart_text(caption))
        if caption_text:
            lines.append(caption_text)

    def table_squeeze(s: str) -> str:
        """è¡¨æ ¼å°ˆç”¨çš„æ–‡æœ¬å£“ç¸®ï¼Œä¿ç•™æ›è¡Œç¬¦ä½†è½‰æ›ç‚ºç©ºæ ¼"""
        # å°‡æ›è¡Œç¬¦æ›¿æ›ç‚ºç©ºæ ¼ï¼Œä¿æŒåœ¨åŒä¸€å€‹å–®å…ƒæ ¼å…§
        s = s.replace('\n', ' ')
        # ç„¶å¾Œé€²è¡Œæ­£å¸¸çš„ç©ºç™½å£“ç¸®
        return re.sub(r"[ \t\u00A0]+", " ", s.strip())

    row_spans: list[dict | None] = []
    rows: list[list[str]] = []

    for tr in table.find_all("tr"):
        if tr.find_parent("table") is not table:
            continue

        current_row: list[str | None] = []

        # apply active rowspans to this row
        for idx in range(len(row_spans)):
            span = row_spans[idx]
            if span:
                current_row.append(span["value"])
                span["rows_left"] -= 1
                if span["rows_left"] == 0:
                    row_spans[idx] = None
            else:
                current_row.append(None)

        cells = tr.find_all(["th", "td"], recursive=False)
        for cell in cells:
            text = table_squeeze(smart_text(cell))
            colspan = _parse_int(cell.get("colspan"), 1)
            rowspan = _parse_int(cell.get("rowspan"), 1)

            # find the first slot that can host this cell (respecting existing rowspans)
            col_idx = 0
            while True:
                # extend row/rowspan buffers when needed
                while col_idx >= len(current_row):
                    current_row.append(None)
                    row_spans.append(None)

                # ensure the span fits starting at col_idx
                fits = True
                for offset in range(colspan):
                    pos = col_idx + offset
                    while pos >= len(current_row):
                        current_row.append(None)
                        row_spans.append(None)
                    if current_row[pos] is not None:
                        col_idx = pos + 1
                        fits = False
                        break
                if fits:
                    break

            # place the cell content across the colspan
            for offset in range(colspan):
                pos = col_idx + offset
                value = text if offset == 0 else ""
                current_row[pos] = value
                if rowspan > 1:
                    row_spans[pos] = {"rows_left": rowspan - 1, "value": value}
                else:
                    if row_spans[pos] is None:
                        continue
                    row_spans[pos] = None

        cleaned = [c if c is not None else "" for c in current_row]
        while cleaned and not cleaned[-1].strip():
            cleaned.pop()
        if any(col.strip() for col in cleaned):
            rows.append(cleaned)

    for row in rows:
        line = " | ".join(row)
        if line.strip():
            lines.append("â€¢ " + line)

    return lines


def process_multicol_table(table: Tag, squeeze) -> list[str]:
    """
    è™•ç† multicol è¡¨æ ¼ï¼Œé€™ç¨®è¡¨æ ¼é€šå¸¸åŒ…å«å¤šæ¬„å…§å®¹ï¼Œæ¯æ¬„æœ‰æ¨™é¡Œå’Œåˆ—è¡¨
    ä¾‹å¦‚ï¼šéŸ³æ¨‚ä½œå“æ¬„ä½ï¼ŒåŒ…å« "éŒ„éŸ³å®¤å°ˆè¼¯"ã€"ç²¾é¸è¼¯"ã€"æ—¥èªä½œå“" ç­‰åˆ†é¡
    """
    lines: list[str] = []
    
    # æŸ¥æ‰¾æ‰€æœ‰çš„ td æ¬„ä½ï¼ŒæŒ‰é †åºè™•ç†æ¯å€‹æ¬„ä½
    for td in table.find_all("td"):
        # æŒ‰æ–‡æª”é †åºè™•ç†è©²æ¬„ä½ä¸­çš„æ‰€æœ‰å­å…ƒç´ 
        for element in td.find_all(["h3", "h4", "h5", "h6", "dl", "ul", "ol"], recursive=True):
            
            # è™•ç†æ¨™é¡Œï¼ˆh3, h4 ç­‰ï¼‰
            if element.name in ["h3", "h4", "h5", "h6"]:
                title = squeeze(smart_text(element))
                if title:
                    # æ·»åŠ å­æ¨™é¡Œï¼ˆå‰é¢æœ‰ç©ºè¡Œåˆ†éš”ï¼‰
                    if lines and lines[-1] != "":
                        lines.append("")
                    lines.append(title)
            
            # è™•ç† dl æ¨™é¡Œ
            elif element.name == "dl":
                for dt in element.find_all("dt"):
                    title = squeeze(smart_text(dt))
                    if title:
                        # æ·»åŠ å­æ¨™é¡Œï¼ˆå‰é¢æœ‰ç©ºè¡Œåˆ†éš”ï¼‰
                        if lines and lines[-1] != "":
                            lines.append("")
                        lines.append(f"### {title}")
            
            # è™•ç†ç„¡åºåˆ—è¡¨ ul
            elif element.name == "ul":
                # åªè™•ç†ç›´æ¥å­é …ç›®ï¼Œé¿å…é‡è¤‡è™•ç†åµŒå¥—åˆ—è¡¨
                for li in element.find_all("li", recursive=False):
                    item = squeeze(smart_text(li))
                    if item:
                        lines.append(f"â€¢ {item}")
            
            # è™•ç†æœ‰åºåˆ—è¡¨ ol
            elif element.name == "ol":
                # åªè™•ç†ç›´æ¥å­é …ç›®ï¼Œé¿å…é‡è¤‡è™•ç†åµŒå¥—åˆ—è¡¨
                for li in element.find_all("li", recursive=False):
                    item = squeeze(smart_text(li))
                    if item:
                        lines.append(f"â€¢ {item}")
    
    return lines


# ===== é€šç”¨æ¨™ç±¤è™•ç†ï¼ˆå­¸å / è—å / æœ¬å / åŸå / åˆ¥å / å¤šèªè¨€ï¼‰ =====

# å¸¸è¦‹æ¨™ç±¤ â†’ BCP-47 èªè¨€ç¢¼ï¼ˆå¯è‡ªè¡Œæ“´å……ï¼‰
LABEL_LANG_MAP = {
    # èªè¨€åç¨±ï¼ˆå«å¸¸è¦‹åˆ¥ç¨±ï¼‰
    "è‹±èª": "en", "è‹±æ–‡": "en", "English": "en",
    "æ—¥èª": "ja", "æ—¥æ–‡": "ja", "Japanese": "ja",
    "éŸ“èª": "ko", "éŸ“æ–‡": "ko",
    "æ³•èª": "fr", "æ³•æ–‡": "fr",
    "å¾·èª": "de", "å¾·æ–‡": "de",
    "è¥¿ç­ç‰™èª": "es", "è¥¿æ–‡": "es", "è¥¿èª": "es",
    "ä¿„èª": "ru", "ä¿„æ–‡": "ru",
    "ç¾©å¤§åˆ©èª": "it", "ç¾©æ–‡": "it", "æ„å¤§åˆ©èª": "it", "æ„æ–‡": "it",
    "è‘¡è„ç‰™èª": "pt", "è‘¡æ–‡": "pt",
    "æ‹‰ä¸èª": "la", "æ‹‰ä¸æ–‡": "la",
    "è¶Šå—èª": "vi", "è¶Šæ–‡": "vi",
    "æ³°èª": "th",
    "é¦¬ä¾†èª": "ms", "é¦¬ä¾†æ–‡": "ms",
    "å°å°¼èª": "id", "å°å°¼æ–‡": "id",
    "ç²µèª": "yue", "å»£æ±è©±": "yue",
    "é–©å—èª": "nan", "è‡ºèª": "nan", "å°èª": "nan", "é–©å—è©±": "nan",
    "å®¢èª": "hak",
    # éèªè¨€ä½†å¸¸è¦‹ï¼šå­¸åï¼ˆå¤šåŠæ‹‰ä¸ï¼‰
    "å­¸å": "la",
}

# æœƒè¢«ç•¶æˆã€Œå€¼ç¼ºå¤±ã€çš„åˆ†éš”ç¬¦ï¼ˆé‡åˆ°å®ƒå€‘å°±åœæ­¢è’é›†ï¼‰
_STOP_CHARS = "ï¼Œ,ã€ï¼›;,)ï¼‰ã€ã€ã€‘ã€‰ã€‚."

def _fetch_langlink_title(title: str, session: requests.Session, lang_code: str) -> Optional[str]:
    """
    ç”¨ MediaWiki Action API è®€å–äº’èªé€£çµæ¨™é¡Œï¼ˆlllang=<code>ï¼‰
    """
    params = {
        "action": "query",
        "prop": "langlinks",
        "titles": title,
        "lllang": lang_code,
        "format": "json",
    }
    try:
        r = http_get_with_backoff(session, API, params=params, timeout=20)
        data = r.json()
        pages = data.get("query", {}).get("pages", {})
        for _, page in pages.items():
            for ll in page.get("langlinks", []) or []:
                if ll.get("lang") == lang_code:
                    return ll.get("*") or ll.get("title")
    except Exception:
        pass
    return None


def _collect_value_after_label(el: Tag, label: str, maxlen: int = 120) -> Optional[str]:
    """
    åœ¨æ®µè½ el è£¡æ‰¾åˆ°ã€Œ<label>ï¼šã€é€™å€‹é‚è¼¯åºåˆ—ï¼ˆå…è¨± label èˆ‡å†’è™Ÿè·¨ç¯€é»ï¼‰ï¼Œ
    ä¹‹å¾ŒæŠŠå€¼ä¸€è·¯æ”¶é›†åˆ°é‡åˆ°åœæ­¢å­—å…ƒç‚ºæ­¢ã€‚
    ä¾‹ï¼š<a>å­¸å</a>ï¼š<i><span lang="la">Oncorhynchus masou formosanus</span></i>
    """
    # 1) æ‰¾åˆ°æ–‡å­—å‰›å¥½ç­‰æ–¼ label çš„ç¯€é»ï¼ˆå¯èƒ½æ˜¯ NavigableString æˆ– <a>/<b>/<span> ç­‰ Tagï¼‰
    anchor: Optional[object] = None
    for d in el.descendants:
        if isinstance(d, NavigableString):
            if d.strip() == label:
                anchor = d
                break
        elif isinstance(d, Tag):
            # åƒ…å–ç´”æ–‡å­—å‰›å¥½ç­‰æ–¼ label çš„ç¯€é»ï¼Œé¿å…æŠ“åˆ°é•·å¥å­ä¸­çš„ç‰‡æ®µ
            if d.get_text("", strip=True) == label:
                anchor = d
                break
    if not anchor:
        return None

    # 2) å¾ anchor å¾€å¾Œæ‰¾ã€Œç¬¬ä¸€å€‹å†’è™Ÿã€ï¼ˆå…è¨±å…¨/åŠå½¢ï¼‰ï¼Œåƒæ‰å®ƒä¹‹å¾Œé–‹å§‹æ”¶é›†å€¼
    it = anchor.next_elements
    saw_colon = False
    parts: list[str] = []

    def cut_at_stop(s: str) -> tuple[str, bool]:
        """æŠŠ s ç åˆ°ç¬¬ä¸€å€‹åœæ­¢å­—å…ƒï¼Œå›å‚³ (æœ‰æ•ˆå…§å®¹, æ˜¯å¦é‡åˆ°åœæ­¢)"""
        s = s.strip()
        if not s:
            return "", False
        for idx, ch in enumerate(s):
            if ch in _STOP_CHARS:
                return s[:idx], True
        return s, False

    # 3) å¯èƒ½å­˜åœ¨é€™ç¨®æƒ…æ³ï¼šlabel çš„**ä¸‹ä¸€å€‹**æ–‡å­—ç¯€é»ä¸€é–‹å§‹å°±æ˜¯ã€Œï¼šã€æˆ–å«ã€Œï¼š...ã€
    for node in it:
        if isinstance(node, NavigableString):
            s = str(node)
            if not saw_colon:
                pos = s.find("ï¼š")
                if pos < 0:
                    pos = s.find(":")  # ä¹Ÿå®¹è¨±åŠå½¢å†’è™Ÿ
                if pos < 0:
                    # é‚„æ²’çœ‹åˆ°å†’è™Ÿï¼Œç•¥é
                    continue
                # åƒæ‰å†’è™Ÿï¼Œå†’è™Ÿå¾Œå¯èƒ½ç•¶å ´å°±æœ‰å€¼
                s = s[pos + 1:]
                saw_colon = True

            # å·²çœ‹åˆ°å†’è™Ÿï¼Œé–‹å§‹æ”¶é›†å€¼ï¼Œç›´åˆ°é‡åˆ°åœæ­¢å­—å…ƒ
            chunk, stop = cut_at_stop(s)
            if chunk:
                parts.append(chunk)
            if stop:
                break

        elif isinstance(node, Tag):
            # Tag å…§çš„æ–‡å­—æœƒåœ¨å…¶ NavigableString å­å­«ç¯€é»è¢«è™•ç†ï¼›
            # ä½†è‹¥æ˜¯ <br>ï¼Œè¦–ç‚ºç¡¬æ›è¡Œä¸æ”¶é›†
            if node.name == "br":
                break

        # å®‰å…¨ä¸Šé™ï¼Œé¿å…ç•°å¸¸é•·åº¦
        if sum(len(p) for p in parts) > maxlen * 2:
            break

    val = " ".join(p for p in parts if p).strip()
    return val[:maxlen] if val else None

def _collect_value_after_marker(el: Tag, marker_text: str, maxlen: int = 120) -> Optional[str]:
    """
    å¾å«æœ‰ marker_textï¼ˆå¦‚ã€Œå­¸åï¼šã€ã€Œè‹±èªï¼šã€ï¼‰çš„æ®µè½ el å…§ï¼Œ
    æ²¿ DOM next_elements é€£çºŒè’é›†æ–‡å­—ï¼Œç›´åˆ°é‡åˆ° _STOP_CHARSã€‚
    """
    needle = None
    for d in el.descendants:
        if isinstance(d, NavigableString):
            s = str(d)
            if marker_text in s:
                needle = d
                break
    if not needle:
        return None

    parts: list[str] = []
    tail = str(needle).split(marker_text, 1)[1]

    def consume(s: str) -> tuple[str, bool]:
        s = s.strip()
        if not s:
            return "", False
        indices = [i for i, ch in enumerate(s) if ch in _STOP_CHARS]
        if indices:
            i = min(indices)
            return s[:i], True
        return s, False

    if tail:
        chunk, stop = consume(tail)
        if chunk:
            parts.append(chunk)
        if stop:
            out = " ".join(p for p in parts if p).strip()
            return out[:maxlen] if out else None

    for node in needle.next_elements:
        if isinstance(node, NavigableString):
            chunk, stop = consume(str(node))
            if chunk:
                parts.append(chunk)
            if stop:
                break
        elif isinstance(node, Tag):
            classes = " ".join(node.get("class", [])).lower()
            if "reference" in classes or "mw-editsection" in classes:
                continue
        if sum(len(p) for p in parts) > maxlen * 1.5:
            break

    out = " ".join(p for p in parts if p).strip(" ")
    return out[:maxlen] if out else None


# æ¨™ç±¤æ ¸å¿ƒé›†åˆï¼ˆèªè¨€ + å¸¸è¦‹åˆ¥åé¡æ¨™ç±¤ï¼‰
_LABEL_CORE_RE = r"(è‹±èª|è‹±æ–‡|English|æ—¥èª|æ—¥æ–‡|Japanese|éŸ“èª|éŸ“æ–‡|æ³•èª|æ³•æ–‡|å¾·èª|å¾·æ–‡|è¥¿ç­ç‰™èª|è¥¿æ–‡|è¥¿èª|ä¿„èª|ä¿„æ–‡|ç¾©å¤§åˆ©èª|ç¾©æ–‡|æ„å¤§åˆ©èª|æ„æ–‡|è‘¡è„ç‰™èª|è‘¡æ–‡|æ‹‰ä¸èª|æ‹‰ä¸æ–‡|è¶Šå—èª|è¶Šæ–‡|æ³°èª|é¦¬ä¾†èª|é¦¬ä¾†æ–‡|å°å°¼èª|å°å°¼æ–‡|ç²µèª|å»£æ±è©±|é–©å—èª|è‡ºèª|å°èª|é–©å—è©±|å®¢èª|å­¸å|è—å|æœ¬å|åŸå|èˆŠç¨±|åˆå|åˆ¥å|åˆ¥ç¨±|å¤–æ–‡)"

# åµæ¸¬ã€Œæ¨™ç±¤ï¼šã€ä¹‹å¾Œæ¥åˆ†éš”ç¬¦ï¼ˆä»£è¡¨ç¼ºå€¼ï¼‰
_MISSING_AFTER_LABEL = re.compile(_LABEL_CORE_RE + r"ï¼š(?=\s*[ï¼Œã€ï¼›)ï¼‰])")

def fill_labels_if_missing(el: Tag, txt: str) -> str:
    """
    è‹¥åµæ¸¬åˆ°ã€Œ<æ¨™ç±¤>ï¼šã€å¾Œç·Šè·Ÿåˆ†éš”ç¬¦ï¼ˆå€¼ç¼ºå¤±ï¼‰ï¼Œå…ˆç”¨ã€Œæ¨™ç±¤åˆ†ç¯€é»ã€æ³•è£œï¼Œ
    è‹¥å¤±æ•—å†ç”¨ã€Œæ•´æ®µ markerã€æ³•è£œã€‚
    """
    def _repl(m: re.Match) -> str:
        label = m.group(1)
        # å…ˆç”¨åŸæ–¹æ³•æ‰¾
        val = _collect_value_after_label(el, label)
        if not val:
            # åŒä¸€ text node çš„æ¨™ç±¤ï¼šå†’è™Ÿæƒ…æ³
            val = _collect_value_after_marker(el, f"{label}ï¼š")
        return f"{label}ï¼š{val}" if val else m.group(0)

    return _MISSING_AFTER_LABEL.sub(_repl, txt)

def ensure_labels_after_marker(text: str, *, title: str, session: requests.Session) -> str:
    """
    å…¨æ–‡å±¤ç´šæœ€çµ‚å…œåº•ï¼šå°ä»ã€Œ<èªè¨€æ¨™ç±¤>ï¼š<ç¼ºå€¼>ã€è€…ï¼Œå˜—è©¦ç”¨ langlinks å–å°æ‡‰èªè¨€æ¨™é¡Œè£œä¸Šã€‚
    åƒ…å° LABEL_LANG_MAP æœ‰å°æ‡‰ç¢¼çš„æ¨™ç±¤ç”Ÿæ•ˆï¼›å…¶ä»–ï¼ˆå¦‚ è—å/æœ¬åï¼‰ä¸åœ¨æ­¤è™•è£œã€‚
    """
    def _repl(m: re.Match) -> str:
        label = m.group(1)
        code = LABEL_LANG_MAP.get(label)
        if not code:
            return m.group(0)
        name = _fetch_langlink_title(title, session, code)
        return f"{label}ï¼š{name}" if name else m.group(0)

    return _MISSING_AFTER_LABEL.sub(_repl, text)


def html_to_text(html: str, exclude_sections: list[str] | None = None) -> str:
    """
    æ®µè½åŒ–è¼¸å‡ºï¼ˆREST / Action çš†å¯ï¼‰ï¼š
    - å™ªéŸ³å…ˆç§»é™¤
    - é€æƒ h2/h3/p/ul/olï¼›é‡åˆ°è¢«æ’é™¤ç« ç¯€çš„ h2 å¾Œç›´åˆ°ä¸‹ä¸€å€‹ h2 å…¨è·³é
    - **H2** ä¸Šä¸‹å„ç•™ 1 ç©ºç™½è¡Œï¼›å…¶é¤˜è¡Œå–®æ›è¡Œ
    - è‹¥åµæ¸¬åˆ°ã€Œ<æ¨™ç±¤>ï¼šã€å¾Œå…§å®¹ç¼ºå¤±ï¼Œå¾è©²æ®µ DOM è£¡è£œæŠ“å€¼
    - æœ€å¾Œå‘¼å« zh_tidy åšæ¨™é»èˆ‡ç©ºç™½æ”¶æ–‚
    """
    soup = BeautifulSoup(html, "html.parser")

    # å™ªéŸ³æ¸…é™¤
    for tag in soup.select("style, script, noscript"):
        tag.decompose()
    for sel in [
        "sup.reference", "span.mw-editsection", "table.infobox", "table.navbox",
        "div.reflist", "ol.references", "div.metadata",
        # æ¸…é™¤ç¶­åŸºç™¾ç§‘è­¦å‘Šæ¡†å’Œç·¨è¼¯æç¤º
        "div.ambox", "div.mbox-small", "div.messagebox", "table.ambox",
        "div.hatnote", "div.dablink", "div.rellink",
        # æ¸…é™¤ infobox ç›¸é—œ
        "table.infobox", "div.infobox", ".infobox",
        # æ¸…é™¤æ¨¡æ¿å’Œç·¨è¼¯ç›¸é—œ
        "div.navbox", "table.navbox", "div.mw-collapsible"
    ]:
        for tag in soup.select(sel):
            tag.decompose()

    root = soup.select_one("div.mw-parser-output") or soup.body or soup
    elements = root.find_all(["h2", "h3", "p", "ul", "ol", "dl", "table"], recursive=True)

    exclude = set(exclude_sections or [])
    lines: list[str] = []
    skipping = False

    def norm_title(s: str) -> str:
        return re.sub(r"\[.*?\]", "", s).strip()

    def squeeze(s: str) -> str:
        return re.sub(r"[ \t\u00A0]+", " ", s.strip())

    for el in elements:
        if el.name == "h2":
            title = norm_title(smart_text(el))
            if any(key in title for key in exclude):
                skipping = True
                continue
            skipping = False
            if lines and lines[-1] != "":
                lines.append("")          # H2 å‰ç©ºè¡Œ
            if title:
                lines.append(squeeze(title))
                lines.append("")          # H2 å¾Œç©ºè¡Œ
            continue

        if el.name == "h3":
            if skipping:
                continue
            # è·³éå·²è¢« multicol è¡¨æ ¼è™•ç†éçš„ h3
            if el.find_parent("table", class_="multicol"):
                continue
            title = norm_title(smart_text(el))
            if title:
                # ç¢ºä¿H3å‰æœ‰é©ç•¶çš„åˆ†éš”ï¼ˆå¦‚æœå‰é¢ä¸æ˜¯ç©ºè¡Œçš„è©±ï¼‰
                if lines and lines[-1] != "":
                    lines.append("")
                lines.append(squeeze(title))
            continue

        if skipping:
            continue

        if el.name == "p":
            txt = squeeze(smart_text(el))
            if txt:
                # æª¢æŸ¥æ˜¯å¦æ˜¯ç°¡çŸ­çš„æ¨™é¡Œå‹æ®µè½ï¼ˆå¦‚ã€Œå€‹äººæ¦®è­½ã€ï¼‰
                is_likely_title = (
                    len(txt) < 50 and 
                    not txt.endswith(('ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?')) and
                    not any(char in txt for char in 'ï¼Œ,ã€ï¼›;:ï¼š')
                )
                
                if is_likely_title:
                    # ç•¶ä½œæ¨™é¡Œè™•ç†
                    lines.append(txt)
                else:
                    # é€šç”¨æ¨™ç±¤è£œå€¼ï¼ˆå°±åœ° DOM æƒæï¼‰
                    txt = fill_labels_if_missing(el, txt)
                    lines.append(txt)
        elif el.name in ("ul", "ol"):
            # è·³éå·²è¢«è¡¨æ ¼è™•ç†éçš„ ul/ol 
            if el.find_parent("table"):
                continue
            items = []
            for li in el.find_all("li", recursive=False):
                li_txt = squeeze(smart_text(li))
                if li_txt:
                    items.append(f"â€¢ {li_txt}")
            if items:
                lines.extend(items)
        elif el.name == "dl":
            # è™•ç†å®šç¾©åˆ—è¡¨ (definition list)
            # è·³éå·²è¢«è¡¨æ ¼è™•ç†éçš„ dl
            if el.find_parent("table"):
                continue
            for child in el.find_all(["dt", "dd"], recursive=False):
                if child.name == "dt":
                    # dt ä½œç‚ºå°æ¨™é¡Œ
                    dt_txt = squeeze(smart_text(child))
                    if dt_txt:
                        lines.append(f"### {dt_txt}")
                elif child.name == "dd":
                    # å¦‚æœ dd åŒ…å«è¡¨æ ¼ï¼Œè·³éå®ƒï¼ˆè¡¨æ ¼æœƒå–®ç¨è™•ç†ï¼‰
                    if child.find("table"):
                        continue
                    # dd ä½œç‚ºå…§å®¹æ®µè½
                    dd_txt = squeeze(smart_text(child))
                    if dd_txt:
                        lines.append(dd_txt)
        elif el.name == "table":
            # åªè™•ç†å…§å®¹è¡¨æ ¼ï¼Œè·³é infoboxã€navboxã€ambox ç­‰
            if el.find_parent("table") is not None:
                continue
            classes = " ".join(el.get("class", [])).lower()
            if any(cls in classes for cls in ["infobox", "navbox", "ambox", "mbox", "messagebox"]):
                continue
            
            # ç‰¹æ®Šè™•ç† multicol è¡¨æ ¼ï¼ˆå¦‚éŸ³æ¨‚ä½œå“æ¬„ä½ï¼‰
            if "multicol" in classes:
                multicol_lines = process_multicol_table(el, squeeze)
                if multicol_lines:
                    lines.extend(multicol_lines)
            else:
                table_lines = table_to_lines(el, squeeze)
                if table_lines:
                    lines.extend(table_lines)

    text = "\n".join(lines)
    
    # å¾Œè™•ç†ï¼šåˆ†é›¢å¯èƒ½é€£åœ¨ä¸€èµ·çš„æ¨™é¡Œ
    text = separate_concatenated_titles(text)
    
    # æ¸…é™¤å­˜æª”å‚™ä»½æ–‡å­—
    text = remove_archive_links(text)
    
    text = zh_tidy(text)
    return text


def separate_concatenated_titles(text: str) -> str:
    """
    åˆ†é›¢å¯èƒ½è¢«é€£åœ¨ä¸€èµ·çš„æ¨™é¡Œï¼Œä¾‹å¦‚ã€Œå½±éŸ³ä½œå“å…¶ä»–éŸ³æ¨‚éŒ„å½±å¸¶ã€-> ã€Œå½±éŸ³ä½œå“\nå…¶ä»–éŸ³æ¨‚éŒ„å½±å¸¶ã€
    """
    # å¸¸è¦‹çš„éœ€è¦åˆ†é›¢çš„æ¨™é¡Œæ¨¡å¼
    title_pairs = [
        ("å½±éŸ³ä½œå“", "å…¶ä»–éŸ³æ¨‚éŒ„å½±å¸¶"),
        ("å€‹äººç”Ÿæ´»", "æ„Ÿæƒ…ç‹€æ³"),
        ("æ¼”è—ç¶“æ­·", "éŸ³æ¨‚ä½œå“"),
        ("ä½œå“åˆ—è¡¨", "éŸ³æ¨‚ä½œå“"),
        ("ç²çè¨˜éŒ„", "å€‹äººæ¦®è­½"),
    ]
    
    for title1, title2 in title_pairs:
        # æŸ¥æ‰¾é€£åœ¨ä¸€èµ·çš„æ¨™é¡Œ
        combined = title1 + title2
        if combined in text:
            # æ›¿æ›ç‚ºåˆ†è¡Œçš„ç‰ˆæœ¬
            text = text.replace(combined, f"{title1}\n\n{title2}")
    
    return text


def remove_archive_links(text: str) -> str:
    """
    ç§»é™¤ç¶­åŸºç™¾ç§‘çš„å­˜æª”å‚™ä»½é€£çµæ–‡å­—
    ä¾‹å¦‚ï¼šï¼ˆé é¢å­˜æª”å‚™ä»½ï¼Œå­˜æ–¼ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨ï¼‰
    """
    # ç§»é™¤å­˜æª”å‚™ä»½æ–‡å­—çš„å¤šç¨®è®Šé«”
    patterns = [
        r"ï¼ˆé é¢å­˜æª”å‚™ä»½ï¼Œå­˜æ–¼ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨ï¼‰",
        r"\(é é¢å­˜æª”å‚™ä»½ï¼Œå­˜æ–¼ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨\)",
        r"ï¼ˆ\s*é é¢å­˜æª”å‚™ä»½\s*ï¼Œ\s*å­˜æ–¼\s*ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨\s*ï¼‰",
        r"\(\s*é é¢å­˜æª”å‚™ä»½\s*ï¼Œ\s*å­˜æ–¼\s*ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨\s*\)",
    ]
    
    for pattern in patterns:
        text = re.sub(pattern, "", text, flags=re.IGNORECASE)
    
    # æ¸…ç†å¤šé¤˜çš„ç©ºè¡Œå’Œç©ºæ ¼
    text = re.sub(r"\n\s*\n\s*\n", "\n\n", text)  # æ¸›å°‘å¤šé¤˜ç©ºè¡Œ
    text = re.sub(r" +", " ", text)  # å£“ç¸®å¤šé¤˜ç©ºæ ¼
    
    return text


# -------------------------------
# ä¸»æµç¨‹ï¼šå–®ç¯‡è™•ç†
# -------------------------------
def process_one(raw: str, kind: str, out_dir: Path, session: requests.Session,
                exclude_sections: list[str], jsonl_file) -> Tuple[str, bool, str]:
    title = url_to_title(raw) if kind == "url" else raw
    txt_dir = out_dir / "txt"

    requested_filename = safe_filename(title)
    requested_path = txt_dir / f"{requested_filename}.txt"
    if requested_path.exists():
        return title, True, "exists"

    # å…ˆ Actionï¼ˆvariant=zh-twï¼‰ï¼Œå¤±æ•—å† RESTï¼ˆå¸¶ Accept-Language: zh-twï¼‰
    html = None
    actual_title = title
    try:
        html, actual_title = fetch_html_action(title, session)
    except Exception:
        html, actual_title = fetch_html_rest(title, session)

    text = html_to_text(html, exclude_sections=exclude_sections)

    redirect_target = detect_redirect(text)
    if redirect_target and redirect_target != title:
        print(f"ğŸ”„ æª¢æ¸¬åˆ°é‡å®šå‘ï¼š{title} -> {redirect_target}")
        try:
            html, actual_title = fetch_html_action(redirect_target, session)
        except Exception:
            try:
                html, actual_title = fetch_html_rest(redirect_target, session)
            except Exception as e:
                raise RuntimeError(f"é‡å®šå‘ç›®æ¨™é é¢æŠ“å–å¤±æ•—: {redirect_target}, éŒ¯èª¤: {e}")

        text = html_to_text(html, exclude_sections=exclude_sections)

    # å…¨æ–‡å…œåº•ï¼šè‹¥ä»æœ‰ã€Œ<èªè¨€æ¨™ç±¤>ï¼š<ç¼ºå€¼>ã€ï¼Œç”¨ langlinks è£œ
    text = ensure_labels_after_marker(text, title=actual_title, session=session)

    final_filename = safe_filename(actual_title)
    out_txt = txt_dir / f"{final_filename}.txt"
    status = "redirect_ok" if redirect_target else "ok"

    if out_txt.exists():
        if redirect_target:
            return actual_title, True, "redirect_exists"
        return actual_title, True, "exists"

    out_txt.parent.mkdir(parents=True, exist_ok=True)
    out_txt.write_text(text, encoding="utf-8")

    rec = {
        "title": actual_title,
        "original_query": title,
        "source_url": f"https://zh.wikipedia.org/wiki/{quote(actual_title, safe='')}",
        "variant": "zh-tw",
        "source_title_lang": "zh-tw",
        "text_length": len(text),
        "text": text,
        "out_file": str(Path("txt") / f"{final_filename}.txt"),
    }
    if redirect_target:
        rec["redirected_from"] = title
        rec["redirected_to"] = actual_title

    if jsonl_file is not None:
        jsonl_file.write(json.dumps(rec, ensure_ascii=False) + "\n")
        jsonl_file.flush()

    return actual_title, True, status


# -------------------------------
# CLI å…¥å£
# -------------------------------
def main():
    ap = argparse.ArgumentParser(description="æ‰¹æ¬¡æŠ“å–ä¸­æ–‡ç¶­åŸºï¼ˆè‡ºç£æ­£é«” zh-TWï¼‰")
    ap.add_argument("--targets", required=True, help="ç›®æ¨™æ¸…å–®è·¯å¾‘ï¼š.txt æˆ– .jsonl")
    ap.add_argument("--out-dir", default="out", help="è¼¸å‡ºè³‡æ–™å¤¾")
    ap.add_argument("--sleep", type=float, default=1.0, help="æ¯ç¯‡ä¹‹é–“çš„å»¶é²ç§’æ•¸ï¼ˆç¦®è²ŒæŠ“å–ï¼‰")
    ap.add_argument("--ua", default=DEFAULT_UA, help="è‡ªè¨‚ User-Agentï¼ˆè«‹å¡«å¯è¯çµ¡è³‡è¨Šï¼‰")
    ap.add_argument(
        "--exclude-sections",
        # zh è®Šé«”æœƒæŠŠã€Œç›¸å…³æ¡ç›®/æ‰©å±•é˜…è¯»ã€è‡ªå‹•è½‰ç‚ºã€Œç›¸é—œæ¢ç›®/æ“´å±•é–±è®€ã€
        default="åƒè€ƒæ›¸ç›®,ç›¸é—œå­¸è¡“ç ”ç©¶æ›¸ç›®,åƒè€ƒä¾†æº,åƒè€ƒè³‡æ–™,å¤–éƒ¨é€£çµ,ç›¸é—œæ¢ç›®,æ“´å±•é–±è®€,å»¶ä¼¸é–±è®€,åƒè¦‹,åƒè€ƒæ–‡ç»,è…³è¨»,è¨»é‡‹,è¨»è§£,æ³¨è§£,å‚™è¨»,é—œè¯é …ç›®,è³‡æ–™ä¾†æº,æ³¨é‡‹,è¨»è…³,æ³¨è…³,é—œé€£é …ç›®",
        help="è¦æ’é™¤çš„ç« ç¯€æ¨™é¡Œï¼ˆä»¥é€—è™Ÿåˆ†éš”ï¼‰",
    )
    args = ap.parse_args()

    targets_path = Path(args.targets)
    out_dir = Path(args.out_dir)
    exclude_sections = [s.strip() for s in args.exclude_sections.split(",") if s.strip()]

    S = requests.Session()
    S.headers.update({
        "User-Agent": args.ua,
        # è®“ REST/Parsoid ä¾è®Šé«”è¼¸å‡ºè‡ºç£æ­£é«”ï¼›èˆ‡ Action API çš„ variant ç›¸è¼”ç›¸æˆ
        "Accept-Language": "zh-tw",
    })

    ok, skip, fail = 0, 0, 0
    failures_log = out_dir / "_failures.jsonl"
    txt_dir = out_dir / "txt"
    json_dir = out_dir / "jsonl"
    all_data_jsonl = json_dir / "all_data.jsonl"

    out_dir.mkdir(parents=True, exist_ok=True)
    json_dir.mkdir(parents=True, exist_ok=True)
    with all_data_jsonl.open("a", encoding="utf-8") as jsonl_file:
        for raw, kind in iter_targets(targets_path):
            try:
                title, success, msg = process_one(raw, kind, out_dir, S, exclude_sections, jsonl_file)
                if msg == "exists":
                    skip += 1
                    print(f"â­ï¸  ç•¥éï¼ˆå·²å­˜åœ¨ï¼‰ï¼š{title}")
                elif msg == "redirect_exists":
                    skip += 1
                    print(f"â­ï¸  ç•¥éï¼ˆé‡å®šå‘ç›®æ¨™å·²å­˜åœ¨ï¼‰ï¼š{title}")
                elif msg == "redirect_ok":
                    ok += 1
                    print(f"âœ… å®Œæˆï¼ˆé‡å®šå‘ï¼‰ï¼š{raw} -> {title}")
                else:
                    ok += 1
                    print(f"âœ… å®Œæˆï¼š{title}")
            except Exception as e:
                fail += 1
                print(f"âŒ  å¤±æ•—ï¼š{raw}  -> {e}")
                failures_log.parent.mkdir(parents=True, exist_ok=True)
                with failures_log.open("a", encoding="utf-8") as f:
                    f.write(json.dumps({"raw": raw, "kind": kind, "error": str(e)}, ensure_ascii=False) + "\n")
            time.sleep(args.sleep)

    print("\n=== çµ±è¨ˆ ===")
    print(f"æˆåŠŸï¼š{ok}  å·²å­˜åœ¨ï¼š{skip}  å¤±æ•—ï¼š{fail}")
    print(f"æ–‡å­—è¼¸å‡ºï¼š{txt_dir.resolve()}")
    print(f"JSONL è¼¸å‡ºï¼š{all_data_jsonl.resolve()}")


if __name__ == "__main__":
    main()
